name: Docs/Standards/Observability Standards
category: general
filename: docs/standards/OBSERVABILITY_STANDARDS.md
nist_controls: []
sections:
  Observability and Monitoring Standards: '**Version:** 1.0.0

    **Last Updated:** January 2025

    **Status:** Active

    **Standard Code:** OBS


    ---


    **Version:** 1.0.0

    **Last Updated:** January 2025

    **Status:** Active'
  Table of Contents: '1. [Observability Principles](#1-observability-principles)

    2. [Metrics and Monitoring](#2-metrics-and-monitoring)

    3. [Distributed Tracing](#3-distributed-tracing)

    4. [Logging Standards](#4-logging-standards)

    5. [Service Level Objectives (SLOs)](#5-service-level-objectives-slos)

    6. [Alerting and Incident Response](#6-alerting-and-incident-response)

    7. [Performance Monitoring](#7-performance-monitoring)

    8. [Infrastructure Observability](#8-infrastructure-observability)


    ---'
  Overview: 'This standard provides comprehensive guidelines and best practices for
    the subject area.

    It aims to ensure consistency, quality, and maintainability across all related
    implementations.'
  1. Observability Principles: '<!-- @nist-controls: [au-2, au-3, au-4, au-5, au-6,
    au-9, si-4] -->


    ### 1.1 Three Pillars of Observability


    #### Metrics, Logs, and Traces **[REQUIRED]**

    ```yaml'
  Observability strategy configuration: "observability:\n  strategy: \"three_pillars_plus_events\"\
    \n\n  metrics:\n    collection_interval: 15s\n    retention:\n      raw: 15d\n\
    \      downsampled_5m: 90d\n      downsampled_1h: 1y\n    cardinality_limit: 1000000\n\
    \n  logs:\n    retention: 30d  # @nist au-4 \"Audit storage capacity\"\n    structured_format:\
    \ json  # @nist au-3 \"Content of audit records\"\n    compression: gzip\n   \
    \ log_level: info  # @nist au-2 \"Audit events\"\n\n  traces:\n    sampling_rate:\
    \ 0.1  # 10% sampling\n    retention: 7d\n    max_trace_duration: 30s\n\n  events:\n\
    \    retention: 90d\n    structured_format: json\n```\n\n#### OpenTelemetry Implementation\
    \ **[REQUIRED]**\n```python"
  observability/telemetry.py: "from opentelemetry import trace, metrics\nfrom opentelemetry.exporter.jaeger.thrift\
    \ import JaegerExporter\nfrom opentelemetry.exporter.prometheus import PrometheusMetricReader\n\
    from opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export\
    \ import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\n\
    from opentelemetry.sdk.resources import Resource\nfrom opentelemetry.instrumentation.auto_instrumentation\
    \ import sitecustomize\nimport logging\nimport time\nfrom typing import Dict,\
    \ Any, Optional\nfrom contextlib import contextmanager\n\nclass ObservabilityManager:\n\
    \    # @nist au-2 \"Comprehensive audit event generation\"\n    # @nist si-4 \"\
    System monitoring implementation\"\n    def __init__(self, service_name: str,\
    \ service_version: str, environment: str):\n        self.service_name = service_name\n\
    \        self.service_version = service_version\n        self.environment = environment\n\
    \n        # Configure resource attributes\n        self.resource = Resource.create({\n\
    \            \"service.name\": service_name,\n            \"service.version\"\
    : service_version,\n            \"deployment.environment\": environment,\n   \
    \         \"telemetry.sdk.language\": \"python\",\n            \"telemetry.sdk.name\"\
    : \"opentelemetry\",\n        })\n\n        self._setup_tracing()\n        self._setup_metrics()\n\
    \        self._setup_logging()\n\n    def _setup_tracing(self):\n        \"\"\"\
    Configure distributed tracing.\"\"\"\n        trace.set_tracer_provider(TracerProvider(resource=self.resource))\n\
    \n        # Jaeger exporter for traces\n        jaeger_exporter = JaegerExporter(\n\
    \            agent_host_name=\"jaeger-agent\",\n            agent_port=6831,\n\
    \        )\n\n        span_processor = BatchSpanProcessor(jaeger_exporter)\n \
    \       trace.get_tracer_provider().add_span_processor(span_processor)\n\n   \
    \     self.tracer = trace.get_tracer(__name__)\n\n    def _setup_metrics(self):\n\
    \        \"\"\"Configure metrics collection.\"\"\"\n        # Prometheus metrics\
    \ reader\n        prometheus_reader = PrometheusMetricReader()\n\n        metrics.set_meter_provider(\n\
    \            MeterProvider(resource=self.resource, metric_readers=[prometheus_reader])\n\
    \        )\n\n        self.meter = metrics.get_meter(__name__)\n\n        # Standard\
    \ application metrics\n        self.request_counter = self.meter.create_counter(\n\
    \            name=\"http_requests_total\",\n            description=\"Total number\
    \ of HTTP requests\",\n            unit=\"1\"\n        )\n\n        self.request_duration\
    \ = self.meter.create_histogram(\n            name=\"http_request_duration_seconds\"\
    ,\n            description=\"HTTP request duration in seconds\",\n           \
    \ unit=\"s\"\n        )\n\n        self.error_counter = self.meter.create_counter(\n\
    \            name=\"application_errors_total\",\n            description=\"Total\
    \ number of application errors\",\n            unit=\"1\"\n        )\n\n    def\
    \ _setup_logging(self):\n        \"\"\"Configure structured logging.\"\"\"\n \
    \       logging.basicConfig(\n            level=logging.INFO,\n            format='{\"\
    timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", \"logger\": \"%(name)s\"\
    , \"message\": \"%(message)s\", \"service\": \"' + self.service_name + '\"}',\n\
    \            datefmt='%Y-%m-%dT%H:%M:%S'\n        )\n\n        self.logger = logging.getLogger(self.service_name)\n\
    \n    @contextmanager\n    def trace_operation(self, operation_name: str, attributes:\
    \ Optional[Dict[str, Any]] = None):\n        \"\"\"Context manager for tracing\
    \ operations.\"\"\"\n        with self.tracer.start_as_current_span(operation_name)\
    \ as span:\n            start_time = time.time()\n\n            # Add standard\
    \ attributes\n            span.set_attribute(\"operation.name\", operation_name)\n\
    \            span.set_attribute(\"service.name\", self.service_name)\n\n     \
    \       # Add custom attributes\n            if attributes:\n                for\
    \ key, value in attributes.items():\n                    span.set_attribute(key,\
    \ value)\n\n            try:\n                yield span\n                span.set_status(trace.Status(trace.StatusCode.OK))\n\
    \            except Exception as e:\n                span.set_status(\n      \
    \              trace.Status(trace.StatusCode.ERROR, str(e))\n                )\n\
    \                span.record_exception(e)\n                self.error_counter.add(1,\
    \ {\"operation\": operation_name, \"error_type\": type(e).__name__})\n       \
    \         raise\n            finally:\n                duration = time.time()\
    \ - start_time\n                span.set_attribute(\"operation.duration\", duration)\n\
    \n    def record_http_request(self, method: str, endpoint: str, status_code: int,\
    \ duration: float):\n        \"\"\"Record HTTP request metrics.\"\"\"\n      \
    \  labels = {\n            \"method\": method,\n            \"endpoint\": endpoint,\n\
    \            \"status_code\": str(status_code)\n        }\n\n        self.request_counter.add(1,\
    \ labels)\n        self.request_duration.record(duration, labels)\n\n        if\
    \ status_code >= 400:\n            self.error_counter.add(1, {**labels, \"error_type\"\
    : \"http_error\"})\n\n    def log_structured(self, level: str, message: str, **kwargs):\n\
    \        \"\"\"Log structured data with context.\"\"\"\n        log_data = {\n\
    \            \"message\": message,\n            \"service\": self.service_name,\n\
    \            \"environment\": self.environment,\n            **kwargs\n      \
    \  }\n\n        # Add trace context if available\n        current_span = trace.get_current_span()\n\
    \        if current_span:\n            span_context = current_span.get_span_context()\n\
    \            log_data[\"trace_id\"] = format(span_context.trace_id, '032x')\n\
    \            log_data[\"span_id\"] = format(span_context.span_id, '016x')\n\n\
    \        getattr(self.logger, level.lower())(str(log_data))"
  Usage example: "import os\n\nlogger = StructuredLogger(\n    service_name=\"user-service\"\
    ,\n    version=\"1.2.0\",\n    environment=os.getenv(\"ENVIRONMENT\", \"development\"\
    )\n)"
  Trace a database operation: "with observability.trace_operation(\"user_lookup\"\
    , {\"user_id\": \"12345\"}) as span:\n    user = database.get_user(\"12345\")\n\
    \    span.set_attribute(\"user.found\", user is not None)\n    if user:\n    \
    \    span.set_attribute(\"user.segment\", user.segment)"
  Record HTTP metrics: observability.record_http_request("GET", "/users/12345", 200,
    0.045)
  Structured logging: "observability.log_structured(\"info\", \"User retrieved successfully\"\
    ,\n                           user_id=\"12345\", operation=\"user_lookup\")\n\
    ```\n\n### 1.2 Observability Strategy\n\n#### Service Instrumentation **[REQUIRED]**\n\
    ```python"
  observability/instrumentation.py: "from functools import wraps\nfrom typing import\
    \ Callable, Any\nimport time\nimport asyncio\n\ndef instrument_function(operation_name:\
    \ str = None):\n    \"\"\"Decorator to automatically instrument functions.\"\"\
    \"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n   \
    \     def sync_wrapper(*args, **kwargs):\n            op_name = operation_name\
    \ or f\"{func.__module__}.{func.__name__}\"\n\n            with observability.trace_operation(op_name)\
    \ as span:\n                span.set_attribute(\"function.name\", func.__name__)\n\
    \                span.set_attribute(\"function.module\", func.__module__)\n\n\
    \                start_time = time.time()\n                try:\n            \
    \        result = func(*args, **kwargs)\n                    span.set_attribute(\"\
    function.success\", True)\n                    return result\n               \
    \ except Exception as e:\n                    span.set_attribute(\"function.success\"\
    , False)\n                    span.set_attribute(\"function.error\", str(e))\n\
    \                    raise\n                finally:\n                    duration\
    \ = time.time() - start_time\n                    span.set_attribute(\"function.duration\"\
    , duration)\n\n        @wraps(func)\n        async def async_wrapper(*args, **kwargs):\n\
    \            op_name = operation_name or f\"{func.__module__}.{func.__name__}\"\
    \n\n            with observability.trace_operation(op_name) as span:\n       \
    \         span.set_attribute(\"function.name\", func.__name__)\n             \
    \   span.set_attribute(\"function.module\", func.__module__)\n               \
    \ span.set_attribute(\"function.async\", True)\n\n                start_time =\
    \ time.time()\n                try:\n                    result = await func(*args,\
    \ **kwargs)\n                    span.set_attribute(\"function.success\", True)\n\
    \                    return result\n                except Exception as e:\n \
    \                   span.set_attribute(\"function.success\", False)\n        \
    \            span.set_attribute(\"function.error\", str(e))\n                \
    \    raise\n                finally:\n                    duration = time.time()\
    \ - start_time\n                    span.set_attribute(\"function.duration\",\
    \ duration)\n\n        return async_wrapper if asyncio.iscoroutinefunction(func)\
    \ else sync_wrapper\n    return decorator"
  Usage examples: 'tracer = trace.get_tracer(__name__)

    advanced_tracing = AdvancedTracing(tracer)'
  2. Metrics and Monitoring: '### 2.1 Prometheus Configuration


    #### Metrics Collection Setup **[REQUIRED]**

    ```yaml'
  prometheus/prometheus.yml: "global:\n  scrape_interval: 15s\n  evaluation_interval:\
    \ 15s\n  external_labels:\n    cluster: 'production'\n    region: 'us-east-1'\n\
    \nrule_files:\n  - \"alert_rules.yml\"\n  - \"recording_rules.yml\"\n\nalerting:\n\
    \  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n\
    \nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets:\
    \ ['localhost:9090']\n\n  - job_name: 'application-metrics'\n    kubernetes_sd_configs:\n\
    \      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n\
    \        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n\
    \        action: replace\n        target_label: __metrics_path__\n        regex:\
    \ (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n\
    \        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement:\
    \ $1:$2\n        target_label: __address__\n      - action: labelmap\n       \
    \ regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n\
    \        action: replace\n        target_label: kubernetes_namespace\n      -\
    \ source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n     \
    \   target_label: kubernetes_pod_name\n\n  - job_name: 'node-exporter'\n    kubernetes_sd_configs:\n\
    \      - role: node\n    relabel_configs:\n      - action: labelmap\n        regex:\
    \ __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n     \
    \   replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n\
    \        regex: (.+)\n        target_label: __metrics_path__\n        replacement:\
    \ /api/v1/nodes/${1}/proxy/metrics\n\n  - job_name: 'kube-state-metrics'\n   \
    \ static_configs:\n      - targets: ['kube-state-metrics:8080']\n\n  - job_name:\
    \ 'cadvisor'\n    kubernetes_sd_configs:\n      - role: node\n    scheme: https\n\
    \    tls_config:\n      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n\
    \    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\
    \    relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n\
    \      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n\
    \      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n \
    \       target_label: __metrics_path__\n        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n\
    ```\n\n#### Recording Rules **[REQUIRED]**\n```yaml"
  prometheus/recording_rules.yml: "groups:\n  - name: application_metrics\n    interval:\
    \ 30s\n    rules:\n      # Request rate by service\n      - record: service:http_requests:rate5m\n\
    \        expr: rate(http_requests_total[5m])\n\n      # Error rate by service\n\
    \      - record: service:http_errors:rate5m\n        expr: rate(http_requests_total{status_code=~\"\
    5..\"}[5m])\n\n      # Error ratio\n      - record: service:http_error_ratio:rate5m\n\
    \        expr: |\n          service:http_errors:rate5m / service:http_requests:rate5m\n\
    \n      # 95th percentile response time\n      - record: service:http_request_duration:p95:5m\n\
    \        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n\
    \n      # 99th percentile response time\n      - record: service:http_request_duration:p99:5m\n\
    \        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))\n\
    \n  - name: infrastructure_metrics\n    interval: 30s\n    rules:\n      # CPU\
    \ utilization by node\n      - record: node:cpu_utilization:avg5m\n        expr:\
    \ |\n          1 - avg by (instance) (\n            rate(node_cpu_seconds_total{mode=\"\
    idle\"}[5m])\n          )\n\n      # Memory utilization by node\n      - record:\
    \ node:memory_utilization:current\n        expr: |\n          1 - (\n        \
    \    node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes\n          )\n\
    \n      # Disk utilization by device\n      - record: node:disk_utilization:current\n\
    \        expr: |\n          1 - (\n            node_filesystem_avail_bytes / node_filesystem_size_bytes\n\
    \          )\n\n  - name: business_metrics\n    interval: 60s\n    rules:\n  \
    \    # Active users in last hour\n      - record: business:active_users:1h\n \
    \       expr: |\n          count by (service) (\n            increase(user_activity_total[1h])\
    \ > 0\n          )\n\n      # Revenue per minute\n      - record: business:revenue:rate1m\n\
    \        expr: rate(order_value_total[1m])\n\n      # Conversion rate\n      -\
    \ record: business:conversion_rate:5m\n        expr: |\n          rate(orders_completed_total[5m])\
    \ /\n          rate(sessions_started_total[5m])\n```\n\n### 2.2 Custom Metrics\
    \ Implementation\n\n#### Application Metrics **[REQUIRED]**\n```python"
  metrics/application_metrics.py: "from prometheus_client import Counter, Histogram,\
    \ Gauge, Summary\nfrom prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n\
    import time\nfrom typing import Dict, List\nfrom functools import wraps\n\nclass\
    \ ApplicationMetrics:\n    def __init__(self, service_name: str):\n        self.service_name\
    \ = service_name\n\n        # HTTP metrics\n        self.http_requests_total =\
    \ Counter(\n            'http_requests_total',\n            'Total HTTP requests',\n\
    \            ['method', 'endpoint', 'status_code', 'service']\n        )\n\n \
    \       self.http_request_duration = Histogram(\n            'http_request_duration_seconds',\n\
    \            'HTTP request duration',\n            ['method', 'endpoint', 'service'],\n\
    \            buckets=(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0,\
    \ 2.5, 5.0, 7.5, 10.0)\n        )\n\n        # Database metrics\n        self.db_operations_total\
    \ = Counter(\n            'database_operations_total',\n            'Total database\
    \ operations',\n            ['operation', 'table', 'status', 'service']\n    \
    \    )\n\n        self.db_operation_duration = Histogram(\n            'database_operation_duration_seconds',\n\
    \            'Database operation duration',\n            ['operation', 'table',\
    \ 'service'],\n            buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25,\
    \ 0.5, 1.0, 2.5, 5.0)\n        )\n\n        # Business metrics\n        self.user_registrations_total\
    \ = Counter(\n            'user_registrations_total',\n            'Total user\
    \ registrations',\n            ['source', 'service']\n        )\n\n        self.active_sessions\
    \ = Gauge(\n            'active_sessions_current',\n            'Current number\
    \ of active sessions',\n            ['service']\n        )\n\n        self.order_value_total\
    \ = Counter(\n            'order_value_total',\n            'Total order value',\n\
    \            ['currency', 'service']\n        )\n\n        # System metrics\n\
    \        self.memory_usage_bytes = Gauge(\n            'process_memory_usage_bytes',\n\
    \            'Process memory usage in bytes',\n            ['service']\n     \
    \   )\n\n        self.error_count = Counter(\n            'application_errors_total',\n\
    \            'Total application errors',\n            ['error_type', 'component',\
    \ 'service']\n        )\n\n    def record_http_request(self, method: str, endpoint:\
    \ str, status_code: int, duration: float):\n        \"\"\"Record HTTP request\
    \ metrics.\"\"\"\n        labels = {\n            'method': method,\n        \
    \    'endpoint': endpoint,\n            'status_code': str(status_code),\n   \
    \         'service': self.service_name\n        }\n\n        self.http_requests_total.labels(**labels).inc()\n\
    \        self.http_request_duration.labels(\n            method=method,\n    \
    \        endpoint=endpoint,\n            service=self.service_name\n        ).observe(duration)\n\
    \n    def record_db_operation(self, operation: str, table: str, duration: float,\
    \ success: bool = True):\n        \"\"\"Record database operation metrics.\"\"\
    \"\n        status = 'success' if success else 'error'\n\n        self.db_operations_total.labels(\n\
    \            operation=operation,\n            table=table,\n            status=status,\n\
    \            service=self.service_name\n        ).inc()\n\n        self.db_operation_duration.labels(\n\
    \            operation=operation,\n            table=table,\n            service=self.service_name\n\
    \        ).observe(duration)\n\n    def record_user_registration(self, source:\
    \ str = 'web'):\n        \"\"\"Record user registration.\"\"\"\n        self.user_registrations_total.labels(\n\
    \            source=source,\n            service=self.service_name\n        ).inc()\n\
    \n    def set_active_sessions(self, count: int):\n        \"\"\"Set current active\
    \ session count.\"\"\"\n        self.active_sessions.labels(service=self.service_name).set(count)\n\
    \n    def record_order_value(self, value: float, currency: str = 'USD'):\n   \
    \     \"\"\"Record order value.\"\"\"\n        self.order_value_total.labels(\n\
    \            currency=currency,\n            service=self.service_name\n     \
    \   ).inc(value)\n\n    def record_error(self, error_type: str, component: str):\n\
    \        \"\"\"Record application error.\"\"\"\n        self.error_count.labels(\n\
    \            error_type=error_type,\n            component=component,\n      \
    \      service=self.service_name\n        ).inc()\n\n    def update_memory_usage(self,\
    \ bytes_used: int):\n        \"\"\"Update memory usage metric.\"\"\"\n       \
    \ self.memory_usage_bytes.labels(service=self.service_name).set(bytes_used)"
  Decorator for automatic HTTP metrics: "def record_http_metrics(metrics: ApplicationMetrics):\n\
    \    def decorator(func):\n        @wraps(func)\n        def wrapper(request,\
    \ *args, **kwargs):\n            start_time = time.time()\n\n            try:\n\
    \                response = func(request, *args, **kwargs)\n                status_code\
    \ = getattr(response, 'status_code', 200)\n            except Exception as e:\n\
    \                status_code = 500\n                metrics.record_error(type(e).__name__,\
    \ func.__name__)\n                raise\n            finally:\n              \
    \  duration = time.time() - start_time\n                metrics.record_http_request(\n\
    \                    method=request.method,\n                    endpoint=request.path,\n\
    \                    status_code=status_code,\n                    duration=duration\n\
    \                )\n\n            return response\n        return wrapper\n  \
    \  return decorator"
  Usage: 'slo_monitor = SLOMonitor(prometheus_client, slo_config)

    report = slo_monitor.generate_slo_report()

    print(json.dumps(report, indent=2))

    ```


    ---'
  3. Distributed Tracing: '### 3.1 Jaeger Configuration


    #### Tracing Setup **[REQUIRED]**

    ```yaml'
  jaeger/jaeger.yml: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: jaeger-configuration\n\
    data:\n  span-storage-type: elasticsearch\n  es-server-urls: http://elasticsearch:9200\n\
    \  es-username: jaeger\n  es-password: password\n  es-num-shards: 5\n  es-num-replicas:\
    \ 1\n  es-index-prefix: jaeger\n\n---\napiVersion: apps/v1\nkind: Deployment\n\
    metadata:\n  name: jaeger-collector\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n\
    \      app: jaeger-collector\n  template:\n    metadata:\n      labels:\n    \
    \    app: jaeger-collector\n    spec:\n      containers:\n      - name: jaeger-collector\n\
    \        image: jaegertracing/jaeger-collector:1.50\n        ports:\n        -\
    \ containerPort: 14269\n        - containerPort: 14268\n        - containerPort:\
    \ 9411\n        env:\n        - name: SPAN_STORAGE_TYPE\n          value: elasticsearch\n\
    \        - name: ES_SERVER_URLS\n          value: http://elasticsearch:9200\n\
    \        - name: COLLECTOR_OTLP_ENABLED\n          value: \"true\"\n        resources:\n\
    \          requests:\n            memory: 256Mi\n            cpu: 100m\n     \
    \     limits:\n            memory: 512Mi\n            cpu: 500m\n\n---\napiVersion:\
    \ apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\nspec:\n  selector:\n\
    \    matchLabels:\n      app: jaeger-agent\n  template:\n    metadata:\n     \
    \ labels:\n        app: jaeger-agent\n    spec:\n      containers:\n      - name:\
    \ jaeger-agent\n        image: jaegertracing/jaeger-agent:1.50\n        ports:\n\
    \        - containerPort: 5775\n          protocol: UDP\n        - containerPort:\
    \ 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol:\
    \ UDP\n        - containerPort: 5778\n          protocol: TCP\n        env:\n\
    \        - name: REPORTER_GRPC_HOST_PORT\n          value: jaeger-collector:14250\n\
    \        resources:\n          requests:\n            memory: 64Mi\n         \
    \   cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 100m\n\
    ```\n\n#### Advanced Tracing Patterns **[REQUIRED]**\n```python"
  tracing/advanced_patterns.py: "from opentelemetry import trace\nfrom opentelemetry.trace.status\
    \ import Status, StatusCode\nfrom opentelemetry.semconv.trace import SpanAttributes\n\
    import asyncio\nimport time\nfrom typing import Dict, Any, List, Optional\nfrom\
    \ contextlib import asynccontextmanager\n\nclass AdvancedTracing:\n    def __init__(self,\
    \ tracer):\n        self.tracer = tracer\n\n    def trace_database_operation(self,\
    \ operation: str, table: str, query: str = None):\n        \"\"\"Context manager\
    \ for database operations.\"\"\"\n        return self.tracer.start_as_current_span(\n\
    \            f\"db.{operation}\",\n            attributes={\n                SpanAttributes.DB_OPERATION:\
    \ operation,\n                SpanAttributes.DB_SQL_TABLE: table,\n          \
    \      SpanAttributes.DB_SYSTEM: \"postgresql\",\n                SpanAttributes.DB_STATEMENT:\
    \ query[:1000] if query else None,  # Truncate long queries\n            }\n \
    \       )\n\n    def trace_http_client(self, method: str, url: str):\n       \
    \ \"\"\"Context manager for HTTP client calls.\"\"\"\n        return self.tracer.start_as_current_span(\n\
    \            f\"http.client.{method.lower()}\",\n            attributes={\n  \
    \              SpanAttributes.HTTP_METHOD: method,\n                SpanAttributes.HTTP_URL:\
    \ url,\n                SpanAttributes.HTTP_SCHEME: \"https\",\n            }\n\
    \        )\n\n    def trace_message_producer(self, queue_name: str, message_id:\
    \ str):\n        \"\"\"Context manager for message queue producers.\"\"\"\n  \
    \      return self.tracer.start_as_current_span(\n            f\"message.send\"\
    ,\n            kind=trace.SpanKind.PRODUCER,\n            attributes={\n     \
    \           \"messaging.system\": \"rabbitmq\",\n                \"messaging.destination\"\
    : queue_name,\n                \"messaging.message_id\": message_id,\n       \
    \         \"messaging.operation\": \"send\",\n            }\n        )\n\n   \
    \ def trace_message_consumer(self, queue_name: str, message_id: str):\n      \
    \  \"\"\"Context manager for message queue consumers.\"\"\"\n        return self.tracer.start_as_current_span(\n\
    \            f\"message.receive\",\n            kind=trace.SpanKind.CONSUMER,\n\
    \            attributes={\n                \"messaging.system\": \"rabbitmq\"\
    ,\n                \"messaging.destination\": queue_name,\n                \"\
    messaging.message_id\": message_id,\n                \"messaging.operation\":\
    \ \"receive\",\n            }\n        )\n\n    @asynccontextmanager\n    async\
    \ def trace_async_operation(self, operation_name: str, **attributes):\n      \
    \  \"\"\"Async context manager for tracing operations.\"\"\"\n        with self.tracer.start_as_current_span(operation_name)\
    \ as span:\n            # Add custom attributes\n            for key, value in\
    \ attributes.items():\n                span.set_attribute(key, value)\n\n    \
    \        start_time = time.time()\n            try:\n                yield span\n\
    \                span.set_status(Status(StatusCode.OK))\n            except Exception\
    \ as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n \
    \               span.record_exception(e)\n                raise\n            finally:\n\
    \                duration = time.time() - start_time\n                span.set_attribute(\"\
    operation.duration\", duration)\n\n    def create_child_span(self, name: str,\
    \ parent_span=None, **attributes):\n        \"\"\"Create a child span with custom\
    \ parent.\"\"\"\n        if parent_span:\n            ctx = trace.set_span_in_context(parent_span)\n\
    \            return self.tracer.start_span(name, context=ctx, attributes=attributes)\n\
    \        return self.tracer.start_span(name, attributes=attributes)\n\n    def\
    \ inject_trace_context(self, headers: Dict[str, str]) -> Dict[str, str]:\n   \
    \     \"\"\"Inject trace context into HTTP headers.\"\"\"\n        from opentelemetry.propagate\
    \ import inject\n        inject(headers)\n        return headers\n\n    def extract_trace_context(self,\
    \ headers: Dict[str, str]):\n        \"\"\"Extract trace context from HTTP headers.\"\
    \"\"\n        from opentelemetry.propagate import extract\n        return extract(headers)"
  Database operation tracing: "async def fetch_user_data(user_id: str):\n    with\
    \ advanced_tracing.trace_database_operation(\"SELECT\", \"users\", f\"SELECT *\
    \ FROM users WHERE id = {user_id}\") as span:\n        span.set_attribute(\"user.id\"\
    , user_id)\n        # Database operation\n        await asyncio.sleep(0.01)  #\
    \ Simulate DB call\n        span.set_attribute(\"db.rows_affected\", 1)\n    \
    \    return {\"id\": user_id, \"name\": \"John Doe\"}"
  HTTP client tracing: "async def call_external_api(endpoint: str):\n    with advanced_tracing.trace_http_client(\"\
    GET\", f\"https://api.example.com{endpoint}\") as span:\n        # HTTP client\
    \ call\n        await asyncio.sleep(0.1)  # Simulate HTTP call\n        span.set_attribute(SpanAttributes.HTTP_STATUS_CODE,\
    \ 200)\n        span.set_attribute(SpanAttributes.HTTP_RESPONSE_SIZE, 1024)\n\
    \        return {\"status\": \"success\"}"
  Async operation tracing: "async def process_user_order(user_id: str, order_data:\
    \ Dict):\n    async with advanced_tracing.trace_async_operation(\n        \"process_order\"\
    ,\n        user_id=user_id,\n        order_value=order_data.get(\"total\", 0)\n\
    \    ) as span:\n        # Fetch user data\n        user = await fetch_user_data(user_id)\n\
    \        span.set_attribute(\"user.segment\", user.get(\"segment\", \"standard\"\
    ))\n\n        # Call payment service\n        payment_result = await call_external_api(\"\
    /payment/process\")\n        span.set_attribute(\"payment.status\", payment_result[\"\
    status\"])\n\n        # Process order\n        await asyncio.sleep(0.05)  # Simulate\
    \ order processing\n        span.set_attribute(\"order.processed\", True)\n\n\
    \        return {\"order_id\": \"12345\", \"status\": \"completed\"}\n```\n\n\
    ### 3.2 Trace Analysis and Optimization\n\n#### Performance Analysis **[REQUIRED]**\n\
    ```python"
  tracing/analysis.py: "import statistics\nfrom typing import List, Dict, Any\nfrom\
    \ dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\n\
    class SpanSummary:\n    operation_name: str\n    service_name: str\n    avg_duration:\
    \ float\n    p95_duration: float\n    p99_duration: float\n    error_rate: float\n\
    \    call_count: int\n\n@dataclass\nclass TraceAnalysis:\n    trace_id: str\n\
    \    total_duration: float\n    span_count: int\n    error_count: int\n    critical_path:\
    \ List[str]\n    bottlenecks: List[str]\n    services_involved: List[str]\n\n\
    class TraceAnalyzer:\n    def __init__(self, jaeger_client):\n        self.jaeger_client\
    \ = jaeger_client\n\n    def analyze_service_performance(self, service_name: str,\
    \ time_range: timedelta) -> List[SpanSummary]:\n        \"\"\"Analyze performance\
    \ metrics for a service.\"\"\"\n        end_time = datetime.now()\n        start_time\
    \ = end_time - time_range\n\n        # Fetch spans from Jaeger\n        spans\
    \ = self.jaeger_client.get_spans(\n            service_name=service_name,\n  \
    \          start_time=start_time,\n            end_time=end_time\n        )\n\n\
    \        # Group spans by operation\n        operations = {}\n        for span\
    \ in spans:\n            op_name = span.operation_name\n            if op_name\
    \ not in operations:\n                operations[op_name] = []\n            operations[op_name].append(span)\n\
    \n        # Calculate metrics for each operation\n        summaries = []\n   \
    \     for op_name, op_spans in operations.items():\n            durations = [s.duration\
    \ for s in op_spans]\n            errors = [s for s in op_spans if s.has_error]\n\
    \n            summary = SpanSummary(\n                operation_name=op_name,\n\
    \                service_name=service_name,\n                avg_duration=statistics.mean(durations),\n\
    \                p95_duration=statistics.quantiles(durations, n=20)[18],  # 95th\
    \ percentile\n                p99_duration=statistics.quantiles(durations, n=100)[98],\
    \  # 99th percentile\n                error_rate=len(errors) / len(op_spans),\n\
    \                call_count=len(op_spans)\n            )\n            summaries.append(summary)\n\
    \n        return sorted(summaries, key=lambda s: s.p95_duration, reverse=True)\n\
    \n    def analyze_trace(self, trace_id: str) -> TraceAnalysis:\n        \"\"\"\
    Analyze a specific trace for performance issues.\"\"\"\n        trace = self.jaeger_client.get_trace(trace_id)\n\
    \        spans = trace.spans\n\n        # Calculate total duration\n        root_span\
    \ = min(spans, key=lambda s: s.start_time)\n        total_duration = max(s.start_time\
    \ + s.duration for s in spans) - root_span.start_time\n\n        # Find critical\
    \ path (longest sequence of spans)\n        critical_path = self._find_critical_path(spans)\n\
    \n        # Identify bottlenecks (spans that take >20% of total time)\n      \
    \  bottlenecks = [\n            s.operation_name for s in spans\n            if\
    \ s.duration > total_duration * 0.2\n        ]\n\n        # Get involved services\n\
    \        services_involved = list(set(s.service_name for s in spans))\n\n    \
    \    # Count errors\n        error_count = len([s for s in spans if s.has_error])\n\
    \n        return TraceAnalysis(\n            trace_id=trace_id,\n            total_duration=total_duration,\n\
    \            span_count=len(spans),\n            error_count=error_count,\n  \
    \          critical_path=critical_path,\n            bottlenecks=bottlenecks,\n\
    \            services_involved=services_involved\n        )\n\n    def _find_critical_path(self,\
    \ spans: List) -> List[str]:\n        \"\"\"Find the critical path through the\
    \ trace.\"\"\"\n        # Build span hierarchy\n        span_children = {}\n \
    \       for span in spans:\n            parent_id = span.parent_span_id\n    \
    \        if parent_id not in span_children:\n                span_children[parent_id]\
    \ = []\n            span_children[parent_id].append(span)\n\n        # Find path\
    \ with maximum cumulative duration\n        def calculate_path_duration(span_id,\
    \ path):\n            children = span_children.get(span_id, [])\n            if\
    \ not children:\n                return path\n\n            # Find child with\
    \ maximum duration\n            max_child = max(children, key=lambda s: s.duration)\n\
    \            path.append(max_child.operation_name)\n            return calculate_path_duration(max_child.span_id,\
    \ path)\n\n        root_span = next(s for s in spans if s.parent_span_id is None)\n\
    \        return calculate_path_duration(root_span.span_id, [root_span.operation_name])\n\
    \n    def detect_anomalies(self, service_name: str) -> List[Dict[str, Any]]:\n\
    \        \"\"\"Detect performance anomalies in traces.\"\"\"\n        # Get recent\
    \ performance baseline\n        baseline_period = timedelta(hours=24)\n      \
    \  recent_period = timedelta(hours=1)\n\n        baseline_summary = self.analyze_service_performance(service_name,\
    \ baseline_period)\n        recent_summary = self.analyze_service_performance(service_name,\
    \ recent_period)\n\n        anomalies = []\n\n        for recent in recent_summary:\n\
    \            # Find corresponding baseline operation\n            baseline = next((b\
    \ for b in baseline_summary if b.operation_name == recent.operation_name), None)\n\
    \            if not baseline:\n                continue\n\n            # Check\
    \ for duration anomalies (>2x baseline)\n            if recent.p95_duration >\
    \ baseline.p95_duration * 2:\n                anomalies.append({\n           \
    \         \"type\": \"duration_anomaly\",\n                    \"operation\":\
    \ recent.operation_name,\n                    \"baseline_p95\": baseline.p95_duration,\n\
    \                    \"recent_p95\": recent.p95_duration,\n                  \
    \  \"increase_factor\": recent.p95_duration / baseline.p95_duration\n        \
    \        })\n\n            # Check for error rate anomalies (>5x baseline)\n \
    \           if recent.error_rate > baseline.error_rate * 5:\n                anomalies.append({\n\
    \                    \"type\": \"error_rate_anomaly\",\n                    \"\
    operation\": recent.operation_name,\n                    \"baseline_error_rate\"\
    : baseline.error_rate,\n                    \"recent_error_rate\": recent.error_rate,\n\
    \                    \"increase_factor\": recent.error_rate / baseline.error_rate\
    \ if baseline.error_rate > 0 else \"infinity\"\n                })\n\n       \
    \ return anomalies\n```\n\n---"
  4. Logging Standards: '<!-- @nist-controls: [au-2, au-3, au-4, au-5, au-6, au-9]
    -->


    ### 4.1 Structured Logging


    #### Log Format Standards **[REQUIRED]**

    ```python'
  logging/structured_logging.py: "import json\nimport logging\nimport time\nimport\
    \ uuid\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\n\
    import traceback\nimport threading\n\nclass StructuredLogger:\n    # @nist au-3\
    \ \"Structured audit record generation\"\n    # @nist au-9 \"Protection of audit\
    \ information\"\n    def __init__(self, service_name: str, version: str, environment:\
    \ str):\n        self.service_name = service_name\n        self.version = version\n\
    \        self.environment = environment\n        self.correlation_id = None\n\
    \        self._local = threading.local()\n\n        # Configure base logger\n\
    \        self.logger = logging.getLogger(service_name)\n        self.logger.setLevel(logging.INFO)\n\
    \n        # Remove default handlers\n        self.logger.handlers.clear()\n\n\
    \        # Add structured handler\n        handler = logging.StreamHandler()\n\
    \        handler.setFormatter(StructuredFormatter())\n        self.logger.addHandler(handler)\n\
    \n    def set_correlation_id(self, correlation_id: str):\n        \"\"\"Set correlation\
    \ ID for request tracking.\"\"\"\n        self._local.correlation_id = correlation_id\n\
    \n    def get_correlation_id(self) -> Optional[str]:\n        \"\"\"Get current\
    \ correlation ID.\"\"\"\n        return getattr(self._local, 'correlation_id',\
    \ None)\n\n    def _build_log_entry(self, level: str, message: str, **kwargs)\
    \ -> Dict[str, Any]:\n        \"\"\"Build structured log entry.\n        @nist\
    \ au-3 \"Content of audit records\"\n        @nist-implements au-3.1 \"Additional\
    \ audit information\"\n        \"\"\"\n        entry = {\n            \"timestamp\"\
    : datetime.utcnow().isoformat() + \"Z\",\n            \"level\": level.upper(),\n\
    \            \"message\": message,\n            \"service\": self.service_name,\n\
    \            \"version\": self.version,\n            \"environment\": self.environment,\n\
    \            \"thread_id\": threading.get_ident(),\n            \"process_id\"\
    : os.getpid(),\n        }\n\n        # Add correlation ID if available\n     \
    \   correlation_id = self.get_correlation_id()\n        if correlation_id:\n \
    \           entry[\"correlation_id\"] = correlation_id\n\n        # Add trace\
    \ context if available\n        from opentelemetry import trace\n        current_span\
    \ = trace.get_current_span()\n        if current_span and current_span.is_recording():\n\
    \            span_context = current_span.get_span_context()\n            entry[\"\
    trace_id\"] = format(span_context.trace_id, '032x')\n            entry[\"span_id\"\
    ] = format(span_context.span_id, '016x')\n\n        # Add custom fields\n    \
    \    if kwargs:\n            entry[\"fields\"] = kwargs\n\n        return entry\n\
    \n    def info(self, message: str, **kwargs):\n        \"\"\"Log info message.\"\
    \"\"\n        entry = self._build_log_entry(\"info\", message, **kwargs)\n   \
    \     self.logger.info(json.dumps(entry))\n\n    def warning(self, message: str,\
    \ **kwargs):\n        \"\"\"Log warning message.\"\"\"\n        entry = self._build_log_entry(\"\
    warning\", message, **kwargs)\n        self.logger.warning(json.dumps(entry))\n\
    \n    def error(self, message: str, exception: Exception = None, **kwargs):\n\
    \        \"\"\"Log error message with optional exception.\n        @nist si-11\
    \ \"Error handling and logging\"\n        @nist au-2 \"Log security-relevant errors\"\
    \n        \"\"\"\n        if exception:\n            kwargs.update({\n       \
    \         \"exception_type\": type(exception).__name__,\n                \"exception_message\"\
    : str(exception),\n                \"stack_trace\": traceback.format_exc()\n \
    \           })\n\n        entry = self._build_log_entry(\"error\", message, **kwargs)\n\
    \        self.logger.error(json.dumps(entry))\n\n    def debug(self, message:\
    \ str, **kwargs):\n        \"\"\"Log debug message.\"\"\"\n        entry = self._build_log_entry(\"\
    debug\", message, **kwargs)\n        self.logger.debug(json.dumps(entry))\n\n\
    \    def audit(self, action: str, user_id: str, resource: str, **kwargs):\n  \
    \      \"\"\"Log audit event.\"\"\"\n        audit_data = {\n            \"action\"\
    : action,\n            \"user_id\": user_id,\n            \"resource\": resource,\n\
    \            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n         \
    \   **kwargs\n        }\n\n        entry = self._build_log_entry(\"audit\", f\"\
    User {user_id} performed {action} on {resource}\", **audit_data)\n        self.logger.info(json.dumps(entry))\n\
    \nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Formatter for structured\
    \ logs.\"\"\"\n\n    def format(self, record):\n        # If the record message\
    \ is already JSON, return as-is\n        try:\n            json.loads(record.getMessage())\n\
    \            return record.getMessage()\n        except (json.JSONDecodeError,\
    \ ValueError):\n            # Fall back to basic structured format\n         \
    \   log_entry = {\n                \"timestamp\": datetime.utcnow().isoformat()\
    \ + \"Z\",\n                \"level\": record.levelname,\n                \"message\"\
    : record.getMessage(),\n                \"logger\": record.name,\n           \
    \     \"module\": record.module,\n                \"function\": record.funcName,\n\
    \                \"line\": record.lineno,\n            }\n\n            if record.exc_info:\n\
    \                log_entry[\"exception\"] = self.formatException(record.exc_info)\n\
    \n            return json.dumps(log_entry)"
  Set correlation ID for request tracking: logger.set_correlation_id(str(uuid.uuid4()))
  Log structured messages: "logger.info(\"User login attempt\", user_id=\"12345\"\
    , ip_address=\"192.168.1.1\")\nlogger.warning(\"Rate limit approaching\", user_id=\"\
    12345\", current_requests=45, limit=50)\n\ntry:\n    # Some operation that might\
    \ fail\n    result = 1 / 0\nexcept ZeroDivisionError as e:\n    logger.error(\"\
    Division by zero error\", exception=e, operation=\"calculate_ratio\")"
  Audit logging: "logger.audit(\"user_login\", user_id=\"12345\", resource=\"authentication_system\"\
    ,\n            ip_address=\"192.168.1.1\", success=True)\n```\n\n### 4.2 Log Aggregation\
    \ and Analysis\n\n#### ELK Stack Configuration **[REQUIRED]**\n```yaml"
  elk/elasticsearch.yml: 'cluster.name: "logging-cluster"

    node.name: "logging-node-1"

    network.host: 0.0.0.0

    http.port: 9200

    discovery.type: single-node'
  Index template for application logs: "index_patterns: [\"app-logs-*\"]\ntemplate:\n\
    \  settings:\n    number_of_shards: 1\n    number_of_replicas: 1\n    index.refresh_interval:\
    \ \"5s\"\n    index.max_result_window: 50000\n  mappings:\n    properties:\n \
    \     timestamp:\n        type: date\n        format: \"strict_date_optional_time||epoch_millis\"\
    \n      level:\n        type: keyword\n      message:\n        type: text\n  \
    \      analyzer: standard\n      service:\n        type: keyword\n      version:\n\
    \        type: keyword\n      environment:\n        type: keyword\n      correlation_id:\n\
    \        type: keyword\n      trace_id:\n        type: keyword\n      span_id:\n\
    \        type: keyword\n      fields:\n        type: object\n        dynamic:\
    \ true\n\n---"
  logstash/pipeline.conf: "input {\n  beats {\n    port => 5044\n  }\n\n  http {\n\
    \    port => 8080\n    codec => json\n  }\n}\n\nfilter {\n  # Parse timestamp\n\
    \  date {\n    match => [ \"timestamp\", \"ISO8601\" ]\n    target => \"@timestamp\"\
    \n  }\n\n  # Extract log level\n  if [level] {\n    mutate {\n      uppercase\
    \ => [ \"level\" ]\n    }\n  }\n\n  # Add index name based on service and date\n\
    \  mutate {\n    add_field => {\n      \"[@metadata][index]\" => \"app-logs-%{service}-%{+YYYY.MM.dd}\"\
    \n    }\n  }\n\n  # Parse exception stack traces\n  if [fields][exception_type]\
    \ {\n    mutate {\n      add_tag => [ \"exception\" ]\n    }\n  }\n\n  # Add geographical\
    \ information for IP addresses\n  if [fields][ip_address] {\n    geoip {\n   \
    \   source => \"[fields][ip_address]\"\n      target => \"geoip\"\n    }\n  }\n\
    }\n\noutput {\n  elasticsearch {\n    hosts => [\"elasticsearch:9200\"]\n    index\
    \ => \"%{[@metadata][index]}\"\n    template_name => \"app-logs\"\n    template_pattern\
    \ => \"app-logs-*\"\n    template_overwrite => true\n  }\n\n  # Output to stdout\
    \ for debugging\n  stdout {\n    codec => rubydebug\n  }\n}\n\n---"
  kibana/kibana.yml: 'server.host: "0.0.0.0"

    elasticsearch.hosts: ["http://elasticsearch:9200"]

    elasticsearch.username: "kibana_system"

    elasticsearch.password: "password"'
  Dashboard configuration: "xpack.reporting.enabled: true\nxpack.security.enabled:\
    \ false\nlogging.level: info\n```\n\n#### Log Analysis Queries **[REQUIRED]**\n\
    ```json\n{\n  \"kibana_queries\": {\n    \"error_rate_by_service\": {\n      \"\
    query\": {\n        \"bool\": {\n          \"must\": [\n            {\"term\"\
    : {\"level\": \"ERROR\"}},\n            {\"range\": {\"@timestamp\": {\"gte\"\
    : \"now-1h\"}}}\n          ]\n        }\n      },\n      \"aggs\": {\n       \
    \ \"services\": {\n          \"terms\": {\"field\": \"service\"},\n          \"\
    aggs\": {\n            \"error_count\": {\"value_count\": {\"field\": \"level\"\
    }}\n          }\n        }\n      }\n    },\n\n    \"slow_requests\": {\n    \
    \  \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\"exists\"\
    : {\"field\": \"fields.duration\"}},\n            {\"range\": {\"fields.duration\"\
    : {\"gte\": 1000}}},\n            {\"range\": {\"@timestamp\": {\"gte\": \"now-1h\"\
    }}}\n          ]\n        }\n      },\n      \"sort\": [{\"fields.duration\":\
    \ {\"order\": \"desc\"}}]\n    },\n\n    \"trace_analysis\": {\n      \"query\"\
    : {\n        \"bool\": {\n          \"must\": [\n            {\"term\": {\"trace_id\"\
    : \"TRACE_ID_HERE\"}}\n          ]\n        }\n      },\n      \"sort\": [{\"\
    @timestamp\": {\"order\": \"asc\"}}]\n    },\n\n    \"user_activity_analysis\"\
    : {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n        \
    \    {\"term\": {\"fields.user_id\": \"USER_ID_HERE\"}},\n            {\"range\"\
    : {\"@timestamp\": {\"gte\": \"now-24h\"}}}\n          ]\n        }\n      },\n\
    \      \"aggs\": {\n        \"activity_timeline\": {\n          \"date_histogram\"\
    : {\n            \"field\": \"@timestamp\",\n            \"calendar_interval\"\
    : \"hour\"\n          }\n        },\n        \"actions\": {\n          \"terms\"\
    : {\"field\": \"fields.action\"}\n        }\n      }\n    }\n  }\n}\n```\n\n---"
  5. Service Level Objectives (SLOs): '### 5.1 SLO Definition Framework


    #### SLO Configuration **[REQUIRED]**

    ```yaml'
  slo/service_slos.yml: "slos:\n  user_service:\n    availability:\n      objective:\
    \ 99.9%\n      measurement_window: 30d\n      error_budget: 0.1%\n      sli_definition:\
    \ |\n        (sum(rate(http_requests_total{service=\"user-service\",status_code!~\"\
    5..\"}[5m])) /\n         sum(rate(http_requests_total{service=\"user-service\"\
    }[5m]))) * 100\n      alerting_threshold: 50%  # Alert when 50% of error budget\
    \ is consumed\n\n    latency:\n      objective: 95%  # 95% of requests < 200ms\n\
    \      threshold: 0.2  # 200ms\n      measurement_window: 30d\n      sli_definition:\
    \ |\n        histogram_quantile(0.95,\n          rate(http_request_duration_seconds_bucket{service=\"\
    user-service\"}[5m])\n        ) < 0.2\n      alerting_threshold: 90%\n\n    throughput:\n\
    \      objective: 1000  # requests per second\n      measurement_window: 1h\n\
    \      sli_definition: |\n        rate(http_requests_total{service=\"user-service\"\
    }[5m])\n      alerting_threshold: 800  # Alert if below 800 RPS\n\n  payment_service:\n\
    \    availability:\n      objective: 99.95%\n      measurement_window: 30d\n \
    \     error_budget: 0.05%\n      sli_definition: |\n        (sum(rate(http_requests_total{service=\"\
    payment-service\",status_code!~\"5..\"}[5m])) /\n         sum(rate(http_requests_total{service=\"\
    payment-service\"}[5m]))) * 100\n\n    latency:\n      objective: 99%  # 99% of\
    \ requests < 500ms\n      threshold: 0.5\n      measurement_window: 30d\n    \
    \  sli_definition: |\n        histogram_quantile(0.99,\n          rate(http_request_duration_seconds_bucket{service=\"\
    payment-service\"}[5m])\n        ) < 0.5\n\nbusiness_slos:\n  user_experience:\n\
    \    page_load_time:\n      objective: 95%  # 95% of page loads < 2s\n      threshold:\
    \ 2.0\n      measurement_window: 24h\n      sli_definition: |\n        histogram_quantile(0.95,\n\
    \          rate(page_load_duration_seconds_bucket[5m])\n        ) < 2.0\n\n  \
    \  checkout_success_rate:\n      objective: 99.5%\n      measurement_window: 7d\n\
    \      sli_definition: |\n        (sum(rate(checkout_completed_total[5m])) /\n\
    \         sum(rate(checkout_started_total[5m]))) * 100\n```\n\n#### SLO Monitoring\
    \ Implementation **[REQUIRED]**\n```python"
  slo/slo_monitoring.py: "import time\nimport statistics\nfrom typing import Dict,\
    \ List, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import\
    \ datetime, timedelta\nfrom enum import Enum\n\nclass SLOStatus(Enum):\n    HEALTHY\
    \ = \"healthy\"\n    WARNING = \"warning\"\n    CRITICAL = \"critical\"\n    BREACH\
    \ = \"breach\"\n\n@dataclass\nclass SLOResult:\n    name: str\n    current_value:\
    \ float\n    objective: float\n    status: SLOStatus\n    error_budget_remaining:\
    \ float\n    measurement_window: timedelta\n    last_updated: datetime\n\n@dataclass\n\
    class ErrorBudget:\n    total_budget: float\n    consumed: float\n    remaining:\
    \ float\n    burn_rate: float  # Current rate of consumption per hour\n\nclass\
    \ SLOMonitor:\n    def __init__(self, prometheus_client, config: Dict[str, Any]):\n\
    \        self.prometheus = prometheus_client\n        self.config = config\n \
    \       self.slo_history = {}\n\n    def evaluate_slo(self, service_name: str,\
    \ slo_name: str) -> SLOResult:\n        \"\"\"Evaluate a specific SLO.\"\"\"\n\
    \        slo_config = self.config['slos'][service_name][slo_name]\n\n        #\
    \ Query Prometheus for SLI value\n        query = slo_config['sli_definition']\n\
    \        result = self.prometheus.query(query)\n\n        if not result:\n   \
    \         raise Exception(f\"No data available for SLO {service_name}.{slo_name}\"\
    )\n\n        current_value = float(result[0]['value'][1])\n        objective =\
    \ slo_config['objective']\n\n        # Calculate error budget\n        if slo_name\
    \ == 'availability':\n            error_budget = self._calculate_availability_error_budget(\n\
    \                current_value, objective, slo_config['measurement_window']\n\
    \            )\n        elif slo_name == 'latency':\n            error_budget\
    \ = self._calculate_latency_error_budget(\n                current_value, objective,\
    \ slo_config['threshold']\n            )\n        else:\n            error_budget\
    \ = ErrorBudget(0, 0, 0, 0)  # Default for other SLO types\n\n        # Determine\
    \ status\n        status = self._determine_slo_status(current_value, objective,\
    \ error_budget, slo_config)\n\n        return SLOResult(\n            name=f\"\
    {service_name}.{slo_name}\",\n            current_value=current_value,\n     \
    \       objective=objective,\n            status=status,\n            error_budget_remaining=error_budget.remaining,\n\
    \            measurement_window=timedelta(days=30),  # From config\n         \
    \   last_updated=datetime.utcnow()\n        )\n\n    def _calculate_availability_error_budget(self,\
    \ current_availability: float,\n                                           objective:\
    \ float, window: str) -> ErrorBudget:\n        \"\"\"Calculate error budget for\
    \ availability SLO.\"\"\"\n        # Convert window to hours\n        window_hours\
    \ = self._parse_time_window(window)\n\n        # Calculate total error budget\
    \ (time that can be unavailable)\n        total_budget_hours = window_hours *\
    \ (1 - objective / 100)\n\n        # Calculate consumed budget\n        consumed_hours\
    \ = window_hours * (1 - current_availability / 100)\n\n        # Calculate remaining\
    \ budget\n        remaining_hours = total_budget_hours - consumed_hours\n    \
    \    remaining_percentage = (remaining_hours / total_budget_hours) * 100 if total_budget_hours\
    \ > 0 else 0\n\n        # Calculate burn rate (simplified - would need historical\
    \ data for accuracy)\n        burn_rate = consumed_hours / window_hours if window_hours\
    \ > 0 else 0\n\n        return ErrorBudget(\n            total_budget=total_budget_hours,\n\
    \            consumed=consumed_hours,\n            remaining=remaining_percentage,\n\
    \            burn_rate=burn_rate\n        )\n\n    def _calculate_latency_error_budget(self,\
    \ current_latency: float,\n                                      objective: float,\
    \ threshold: float) -> ErrorBudget:\n        \"\"\"Calculate error budget for\
    \ latency SLO.\"\"\"\n        # For latency SLOs, error budget is based on percentage\
    \ of requests above threshold\n        if current_latency <= threshold:\n    \
    \        remaining = 100.0\n            consumed = 0.0\n        else:\n      \
    \      # Simplified calculation - would need more sophisticated logic in practice\n\
    \            overage = (current_latency - threshold) / threshold\n           \
    \ consumed = min(overage * 100, 100)\n            remaining = max(100 - consumed,\
    \ 0)\n\n        return ErrorBudget(\n            total_budget=100.0,\n       \
    \     consumed=consumed,\n            remaining=remaining,\n            burn_rate=consumed\
    \  # Simplified\n        )\n\n    def _determine_slo_status(self, current_value:\
    \ float, objective: float,\n                            error_budget: ErrorBudget,\
    \ config: Dict) -> SLOStatus:\n        \"\"\"Determine SLO status based on current\
    \ performance and error budget.\"\"\"\n\n        # Check if SLO is breached\n\
    \        if (config.get('type') == 'availability' and current_value < objective)\
    \ or \\\n           (config.get('type') == 'latency' and current_value > config.get('threshold',\
    \ 0)):\n            return SLOStatus.BREACH\n\n        # Check error budget consumption\n\
    \        if error_budget.remaining <= 10:  # Less than 10% error budget remaining\n\
    \            return SLOStatus.CRITICAL\n        elif error_budget.remaining <=\
    \ 25:  # Less than 25% error budget remaining\n            return SLOStatus.WARNING\n\
    \        else:\n            return SLOStatus.HEALTHY\n\n    def _parse_time_window(self,\
    \ window: str) -> float:\n        \"\"\"Parse time window string to hours.\"\"\
    \"\n        if window.endswith('d'):\n            return float(window[:-1]) *\
    \ 24\n        elif window.endswith('h'):\n            return float(window[:-1])\n\
    \        elif window.endswith('m'):\n            return float(window[:-1]) / 60\n\
    \        else:\n            return 24.0  # Default to 24 hours\n\n    def evaluate_all_slos(self)\
    \ -> List[SLOResult]:\n        \"\"\"Evaluate all configured SLOs.\"\"\"\n   \
    \     results = []\n\n        for service_name, service_slos in self.config['slos'].items():\n\
    \            for slo_name in service_slos:\n                try:\n           \
    \         result = self.evaluate_slo(service_name, slo_name)\n               \
    \     results.append(result)\n                except Exception as e:\n       \
    \             print(f\"Failed to evaluate SLO {service_name}.{slo_name}: {e}\"\
    )\n\n        return results\n\n    def generate_slo_report(self) -> Dict[str,\
    \ Any]:\n        \"\"\"Generate comprehensive SLO report.\"\"\"\n        results\
    \ = self.evaluate_all_slos()\n\n        # Categorize by status\n        status_counts\
    \ = {status: 0 for status in SLOStatus}\n        for result in results:\n    \
    \        status_counts[result.status] += 1\n\n        # Find SLOs at risk\n  \
    \      at_risk = [r for r in results if r.status in [SLOStatus.WARNING, SLOStatus.CRITICAL]]\n\
    \        breached = [r for r in results if r.status == SLOStatus.BREACH]\n\n \
    \       return {\n            \"report_timestamp\": datetime.utcnow().isoformat(),\n\
    \            \"total_slos\": len(results),\n            \"status_summary\": {status.value:\
    \ count for status, count in status_counts.items()},\n            \"slos_at_risk\"\
    : len(at_risk),\n            \"slos_breached\": len(breached),\n            \"\
    detailed_results\": [\n                {\n                    \"name\": r.name,\n\
    \                    \"status\": r.status.value,\n                    \"current_value\"\
    : r.current_value,\n                    \"objective\": r.objective,\n        \
    \            \"error_budget_remaining\": r.error_budget_remaining\n          \
    \      }\n                for r in results\n            ],\n            \"recommendations\"\
    : self._generate_recommendations(results)\n        }\n\n    def _generate_recommendations(self,\
    \ results: List[SLOResult]) -> List[str]:\n        \"\"\"Generate recommendations\
    \ based on SLO status.\"\"\"\n        recommendations = []\n\n        breached\
    \ = [r for r in results if r.status == SLOStatus.BREACH]\n        critical = [r\
    \ for r in results if r.status == SLOStatus.CRITICAL]\n\n        if breached:\n\
    \            recommendations.append(f\"URGENT: {len(breached)} SLOs are breached.\
    \ Immediate action required.\")\n\n        if critical:\n            recommendations.append(f\"\
    WARNING: {len(critical)} SLOs are critical. Error budget nearly exhausted.\")\n\
    \n        # Check for patterns\n        availability_issues = [r for r in results\
    \ if 'availability' in r.name and r.status != SLOStatus.HEALTHY]\n        if len(availability_issues)\
    \ > 1:\n            recommendations.append(\"Multiple availability SLOs affected.\
    \ Check for infrastructure issues.\")\n\n        latency_issues = [r for r in\
    \ results if 'latency' in r.name and r.status != SLOStatus.HEALTHY]\n        if\
    \ len(latency_issues) > 1:\n            recommendations.append(\"Multiple latency\
    \ SLOs affected. Check for performance degradation.\")\n\n        return recommendations"
  6. Alerting and Incident Response: '### 6.1 Alert Configuration


    #### Prometheus Alerting Rules **[REQUIRED]**

    ```yaml'
  prometheus/alert_rules.yml: "groups:\n  - name: SLO_Alerts\n    rules:\n      #\
    \ High Error Rate\n      - alert: HighErrorRate\n        expr: |\n          (\n\
    \            rate(http_requests_total{status_code=~\"5..\"}[5m]) /\n         \
    \   rate(http_requests_total[5m])\n          ) * 100 > 1\n        for: 2m\n  \
    \      labels:\n          severity: critical\n          team: platform\n     \
    \     service: \"{{ $labels.service }}\"\n        annotations:\n          summary:\
    \ \"High error rate detected for {{ $labels.service }}\"\n          description:\
    \ |\n            Error rate is {{ $value | humanizePercentage }} for service {{\
    \ $labels.service }}.\n            This is above the 1% threshold for 2 minutes.\n\
    \          runbook_url: \"https://runbooks.company.com/high-error-rate\"\n   \
    \       dashboard_url: \"https://grafana.company.com/d/service-overview?var-service={{\
    \ $labels.service }}\"\n\n      # High Latency\n      - alert: HighLatency\n \
    \       expr: |\n          histogram_quantile(0.95,\n            rate(http_request_duration_seconds_bucket[5m])\n\
    \          ) > 0.5\n        for: 5m\n        labels:\n          severity: warning\n\
    \          team: platform\n          service: \"{{ $labels.service }}\"\n    \
    \    annotations:\n          summary: \"High latency detected for {{ $labels.service\
    \ }}\"\n          description: |\n            95th percentile latency is {{ $value\
    \ }}s for service {{ $labels.service }}.\n            This is above the 500ms\
    \ threshold for 5 minutes.\n\n      # SLO Error Budget Consumption\n      - alert:\
    \ SLOErrorBudgetCritical\n        expr: |\n          (1 - slo_availability_ratio)\
    \ * 100 > 0.05  # >50% of 0.1% error budget\n        for: 1m\n        labels:\n\
    \          severity: critical\n          team: platform\n          slo_type: availability\n\
    \        annotations:\n          summary: \"SLO error budget critically low\"\n\
    \          description: |\n            Error budget consumption is {{ $value }}%\
    \ for {{ $labels.service }}.\n            At current rate, error budget will be\
    \ exhausted in less than 1 day.\n\n      # Service Down\n      - alert: ServiceDown\n\
    \        expr: up == 0\n        for: 1m\n        labels:\n          severity:\
    \ critical\n          team: platform\n        annotations:\n          summary:\
    \ \"Service {{ $labels.instance }} is down\"\n          description: |\n     \
    \       Service {{ $labels.instance }} has been down for more than 1 minute.\n\
    \n  - name: Infrastructure_Alerts\n    rules:\n      # High CPU Usage\n      -\
    \ alert: HighCPUUsage\n        expr: |\n          (1 - avg by(instance) (rate(node_cpu_seconds_total{mode=\"\
    idle\"}[5m]))) * 100 > 80\n        for: 5m\n        labels:\n          severity:\
    \ warning\n          team: infrastructure\n        annotations:\n          summary:\
    \ \"High CPU usage on {{ $labels.instance }}\"\n          description: |\n   \
    \         CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance\
    \ }}.\n\n      # High Memory Usage\n      - alert: HighMemoryUsage\n        expr:\
    \ |\n          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))\
    \ * 100 > 90\n        for: 5m\n        labels:\n          severity: critical\n\
    \          team: infrastructure\n        annotations:\n          summary: \"High\
    \ memory usage on {{ $labels.instance }}\"\n          description: |\n       \
    \     Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance\
    \ }}.\n\n      # Disk Space Low\n      - alert: DiskSpaceLow\n        expr: |\n\
    \          (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) *\
    \ 100 > 85\n        for: 5m\n        labels:\n          severity: warning\n  \
    \        team: infrastructure\n        annotations:\n          summary: \"Low\
    \ disk space on {{ $labels.instance }}\"\n          description: |\n         \
    \   Disk usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}:{{\
    \ $labels.mountpoint }}.\n\n  - name: Business_Alerts\n    rules:\n      # Low\
    \ Conversion Rate\n      - alert: LowConversionRate\n        expr: |\n       \
    \   (\n            rate(orders_completed_total[5m]) /\n            rate(sessions_started_total[5m])\n\
    \          ) * 100 < 2\n        for: 10m\n        labels:\n          severity:\
    \ warning\n          team: business\n        annotations:\n          summary:\
    \ \"Conversion rate is critically low\"\n          description: |\n          \
    \  Conversion rate is {{ $value | humanizePercentage }}, below 2% threshold.\n\
    \n      # Payment Failures\n      - alert: HighPaymentFailureRate\n        expr:\
    \ |\n          (\n            rate(payment_failures_total[5m]) /\n           \
    \ rate(payment_attempts_total[5m])\n          ) * 100 > 5\n        for: 2m\n \
    \       labels:\n          severity: critical\n          team: payments\n    \
    \    annotations:\n          summary: \"High payment failure rate\"\n        \
    \  description: |\n            Payment failure rate is {{ $value | humanizePercentage\
    \ }}, above 5% threshold.\n```\n\n### 6.2 Incident Response Automation\n\n####\
    \ AlertManager Configuration **[REQUIRED]**\n```yaml"
  alertmanager/alertmanager.yml: "global:\n  smtp_smarthost: 'localhost:587'\n  smtp_from:\
    \ 'alerts@company.com'\n  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'"
  Inhibition rules - suppress certain alerts when others are firing: "inhibit_rules:\n\
    \  - source_match:\n      severity: 'critical'\n    target_match:\n      severity:\
    \ 'warning'\n    equal: ['alertname', 'service', 'instance']"
  Routing rules: "route:\n  group_by: ['alertname', 'service']\n  group_wait: 30s\n\
    \  group_interval: 5m\n  repeat_interval: 4h\n  receiver: 'default-receiver'\n\
    \n  routes:\n    # Critical alerts go to on-call immediately\n    - match:\n \
    \       severity: critical\n      receiver: 'critical-alerts'\n      group_wait:\
    \ 10s\n      repeat_interval: 5m\n\n    # SLO alerts\n    - match:\n        team:\
    \ platform\n      receiver: 'platform-team'\n\n    # Business alerts\n    - match:\n\
    \        team: business\n      receiver: 'business-team'\n\n    # Infrastructure\
    \ alerts\n    - match:\n        team: infrastructure\n      receiver: 'infrastructure-team'\n\
    \nreceivers:\n  - name: 'default-receiver'\n    email_configs:\n      - to: 'team@company.com'\n\
    \        subject: 'Alert: {{ .GroupLabels.alertname }}'\n        body: |\n   \
    \       {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n   \
    \       Description: {{ .Annotations.description }}\n          {{ end }}\n\n \
    \ - name: 'critical-alerts'\n    slack_configs:\n      - channel: '#critical-alerts'\n\
    \        color: 'danger'\n        title: 'CRITICAL: {{ .GroupLabels.alertname\
    \ }}'\n        text: |\n          {{ range .Alerts }}\n          *Alert:* {{ .Annotations.summary\
    \ }}\n          *Description:* {{ .Annotations.description }}\n          *Runbook:*\
    \ {{ .Annotations.runbook_url }}\n          *Dashboard:* {{ .Annotations.dashboard_url\
    \ }}\n          {{ end }}\n    pagerduty_configs:\n      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'\n\
    \        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary\
    \ }}'\n\n  - name: 'platform-team'\n    slack_configs:\n      - channel: '#platform-alerts'\n\
    \        color: '{{ if eq .Status \"firing\" }}warning{{ else }}good{{ end }}'\n\
    \        title: '{{ .GroupLabels.alertname }}'\n        text: |\n          {{\
    \ range .Alerts }}\n          *Service:* {{ .Labels.service }}\n          *Summary:*\
    \ {{ .Annotations.summary }}\n          *Description:* {{ .Annotations.description\
    \ }}\n          {{ end }}\n\n  - name: 'business-team'\n    slack_configs:\n \
    \     - channel: '#business-alerts'\n        color: 'warning'\n        title:\
    \ 'Business Metric Alert: {{ .GroupLabels.alertname }}'\n\n  - name: 'infrastructure-team'\n\
    \    email_configs:\n      - to: 'infrastructure@company.com'\n        subject:\
    \ 'Infrastructure Alert: {{ .GroupLabels.alertname }}'\n```\n\n#### Incident Response\
    \ Automation **[REQUIRED]**\n```python"
  incident/response_automation.py: "import requests\nimport json\nimport time\nfrom\
    \ typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass,\
    \ asdict\nfrom datetime import datetime\nfrom enum import Enum\n\nclass IncidentSeverity(Enum):\n\
    \    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL\
    \ = \"critical\"\n\nclass IncidentStatus(Enum):\n    OPEN = \"open\"\n    INVESTIGATING\
    \ = \"investigating\"\n    IDENTIFIED = \"identified\"\n    MONITORING = \"monitoring\"\
    \n    RESOLVED = \"resolved\"\n\n@dataclass\nclass Incident:\n    id: str\n  \
    \  title: str\n    description: str\n    severity: IncidentSeverity\n    status:\
    \ IncidentStatus\n    affected_services: List[str]\n    created_at: datetime\n\
    \    updated_at: datetime\n    assigned_to: Optional[str] = None\n    resolution_notes:\
    \ Optional[str] = None\n\nclass IncidentManager:\n    def __init__(self, config:\
    \ Dict[str, Any]):\n        self.config = config\n        self.incidents = {}\
    \  # In practice, this would be a database\n        self.notification_channels\
    \ = {\n            'slack': SlackNotifier(config['slack']),\n            'pagerduty':\
    \ PagerDutyNotifier(config['pagerduty']),\n            'email': EmailNotifier(config['email'])\n\
    \        }\n\n    def create_incident(self, alert_data: Dict[str, Any]) -> Incident:\n\
    \        \"\"\"Create a new incident from alert data.\"\"\"\n        # Generate\
    \ incident ID\n        incident_id = f\"INC-{int(time.time())}\"\n\n        #\
    \ Determine severity from alert\n        severity = self._map_alert_severity(alert_data.get('labels',\
    \ {}).get('severity', 'warning'))\n\n        # Extract affected services\n   \
    \     affected_services = [alert_data.get('labels', {}).get('service', 'unknown')]\n\
    \n        incident = Incident(\n            id=incident_id,\n            title=alert_data.get('annotations',\
    \ {}).get('summary', 'Unknown Alert'),\n            description=alert_data.get('annotations',\
    \ {}).get('description', ''),\n            severity=severity,\n            status=IncidentStatus.OPEN,\n\
    \            affected_services=affected_services,\n            created_at=datetime.utcnow(),\n\
    \            updated_at=datetime.utcnow()\n        )\n\n        self.incidents[incident_id]\
    \ = incident\n\n        # Trigger automated response\n        self._trigger_incident_response(incident,\
    \ alert_data)\n\n        return incident\n\n    def _map_alert_severity(self,\
    \ alert_severity: str) -> IncidentSeverity:\n        \"\"\"Map alert severity\
    \ to incident severity.\"\"\"\n        mapping = {\n            'critical': IncidentSeverity.CRITICAL,\n\
    \            'warning': IncidentSeverity.MEDIUM,\n            'info': IncidentSeverity.LOW\n\
    \        }\n        return mapping.get(alert_severity.lower(), IncidentSeverity.MEDIUM)\n\
    \n    def _trigger_incident_response(self, incident: Incident, alert_data: Dict[str,\
    \ Any]):\n        \"\"\"Trigger automated incident response actions.\"\"\"\n\n\
    \        # 1. Send notifications based on severity\n        if incident.severity\
    \ == IncidentSeverity.CRITICAL:\n            self._send_critical_notifications(incident,\
    \ alert_data)\n        else:\n            self._send_standard_notifications(incident,\
    \ alert_data)\n\n        # 2. Auto-assign if possible\n        assignee = self._determine_assignee(incident,\
    \ alert_data)\n        if assignee:\n            self.assign_incident(incident.id,\
    \ assignee)\n\n        # 3. Create war room if critical\n        if incident.severity\
    \ == IncidentSeverity.CRITICAL:\n            self._create_war_room(incident)\n\
    \n        # 4. Update status page if public-facing service\n        if self._is_public_facing_service(incident.affected_services):\n\
    \            self._update_status_page(incident)\n\n        # 5. Trigger automated\
    \ remediation if available\n        self._attempt_auto_remediation(incident, alert_data)\n\
    \n    def _send_critical_notifications(self, incident: Incident, alert_data: Dict[str,\
    \ Any]):\n        \"\"\"Send notifications for critical incidents.\"\"\"\n   \
    \     # PagerDuty for immediate response\n        self.notification_channels['pagerduty'].trigger_incident(\n\
    \            incident_key=incident.id,\n            description=incident.title,\n\
    \            details=alert_data\n        )\n\n        # Slack to critical channel\n\
    \        self.notification_channels['slack'].send_critical_alert(\n          \
    \  incident=incident,\n            channel='#critical-incidents',\n          \
    \  alert_data=alert_data\n        )\n\n        # Email to leadership for SEV1\n\
    \        if incident.severity == IncidentSeverity.CRITICAL:\n            self.notification_channels['email'].send_incident_notification(\n\
    \                incident=incident,\n                recipients=['leadership@company.com'],\n\
    \                template='critical_incident'\n            )\n\n    def _send_standard_notifications(self,\
    \ incident: Incident, alert_data: Dict[str, Any]):\n        \"\"\"Send standard\
    \ notifications for non-critical incidents.\"\"\"\n        # Slack to team channel\n\
    \        team = alert_data.get('labels', {}).get('team', 'platform')\n       \
    \ channel = f\"#{team}-alerts\"\n\n        self.notification_channels['slack'].send_alert(\n\
    \            incident=incident,\n            channel=channel,\n            alert_data=alert_data\n\
    \        )\n\n    def _determine_assignee(self, incident: Incident, alert_data:\
    \ Dict[str, Any]) -> Optional[str]:\n        \"\"\"Determine who should be assigned\
    \ to the incident.\"\"\"\n        # Get team from alert labels\n        team =\
    \ alert_data.get('labels', {}).get('team')\n        service = alert_data.get('labels',\
    \ {}).get('service')\n\n        # Lookup on-call engineer\n        oncall_schedule\
    \ = self.config.get('oncall_schedules', {})\n        if team in oncall_schedule:\n\
    \            return self._get_oncall_engineer(team)\n        elif service in self.config.get('service_owners',\
    \ {}):\n            return self.config['service_owners'][service]\n\n        return\
    \ None\n\n    def _get_oncall_engineer(self, team: str) -> Optional[str]:\n  \
    \      \"\"\"Get current on-call engineer for team.\"\"\"\n        # In practice,\
    \ this would integrate with PagerDuty or similar\n        oncall_api = self.config.get('oncall_schedules',\
    \ {}).get(team, {})\n        if oncall_api:\n            # Mock implementation\n\
    \            return \"oncall-engineer@company.com\"\n        return None\n\n \
    \   def _create_war_room(self, incident: Incident):\n        \"\"\"Create war\
    \ room for critical incident coordination.\"\"\"\n        # Create Slack channel\n\
    \        channel_name = f\"incident-{incident.id.lower()}\"\n\n        self.notification_channels['slack'].create_channel(\n\
    \            name=channel_name,\n            purpose=f\"War room for {incident.title}\"\
    ,\n            initial_members=['@incident-commander', '@on-call-engineer']\n\
    \        )\n\n        # Pin incident information\n        incident_summary = f\"\
    \"\"\n        \U0001F6A8 **INCIDENT {incident.id}** \U0001F6A8\n        **Title:**\
    \ {incident.title}\n        **Severity:** {incident.severity.value.upper()}\n\
    \        **Affected Services:** {', '.join(incident.affected_services)}\n    \
    \    **Created:** {incident.created_at.strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n\
    \        **Next Steps:**\n        1. Incident Commander: Assess impact and coordinate\
    \ response\n        2. On-call Engineer: Begin investigation and mitigation\n\
    \        3. Communications: Prepare customer communications if needed\n      \
    \  \"\"\"\n\n        self.notification_channels['slack'].pin_message(\n      \
    \      channel=channel_name,\n            message=incident_summary\n        )\n\
    \n    def _is_public_facing_service(self, services: List[str]) -> bool:\n    \
    \    \"\"\"Check if any affected services are public-facing.\"\"\"\n        public_services\
    \ = self.config.get('public_services', [])\n        return any(service in public_services\
    \ for service in services)\n\n    def _update_status_page(self, incident: Incident):\n\
    \        \"\"\"Update external status page for public incidents.\"\"\"\n     \
    \   status_page_api = self.config.get('status_page', {})\n        if not status_page_api:\n\
    \            return\n\n        # Map incident severity to status page impact\n\
    \        impact_mapping = {\n            IncidentSeverity.CRITICAL: 'critical',\n\
    \            IncidentSeverity.HIGH: 'major',\n            IncidentSeverity.MEDIUM:\
    \ 'minor',\n            IncidentSeverity.LOW: 'maintenance'\n        }\n\n   \
    \     impact = impact_mapping[incident.severity]\n\n        # Create status page\
    \ incident\n        status_incident = {\n            'name': incident.title,\n\
    \            'status': 'investigating',\n            'impact': impact,\n     \
    \       'component_ids': self._get_component_ids(incident.affected_services),\n\
    \            'body': incident.description or 'We are investigating reports of\
    \ issues with our service.'\n        }\n\n        # Post to status page API (implementation\
    \ would depend on provider)\n        # self._post_to_status_page(status_incident)\n\
    \n    def _attempt_auto_remediation(self, incident: Incident, alert_data: Dict[str,\
    \ Any]):\n        \"\"\"Attempt automated remediation based on alert type.\"\"\
    \"\n        alert_name = alert_data.get('labels', {}).get('alertname', '')\n\n\
    \        remediation_actions = {\n            'HighMemoryUsage': self._restart_high_memory_pods,\n\
    \            'ServiceDown': self._restart_failed_service,\n            'HighErrorRate':\
    \ self._enable_circuit_breaker,\n            'HighLatency': self._scale_up_service\n\
    \        }\n\n        if alert_name in remediation_actions:\n            action\
    \ = remediation_actions[alert_name]\n            try:\n                result\
    \ = action(incident, alert_data)\n                if result:\n               \
    \     self._add_incident_note(\n                        incident.id,\n       \
    \                 f\"Automated remediation attempted: {alert_name}\"\n       \
    \             )\n            except Exception as e:\n                self._add_incident_note(\n\
    \                    incident.id,\n                    f\"Automated remediation\
    \ failed: {str(e)}\"\n                )\n\n    def _restart_high_memory_pods(self,\
    \ incident: Incident, alert_data: Dict) -> bool:\n        \"\"\"Restart pods with\
    \ high memory usage.\"\"\"\n        # Implementation would use Kubernetes API\n\
    \        return True\n\n    def _restart_failed_service(self, incident: Incident,\
    \ alert_data: Dict) -> bool:\n        \"\"\"Restart failed service.\"\"\"\n  \
    \      # Implementation would use orchestration system API\n        return True\n\
    \n    def _enable_circuit_breaker(self, incident: Incident, alert_data: Dict)\
    \ -> bool:\n        \"\"\"Enable circuit breaker for service with high error rate.\"\
    \"\"\n        # Implementation would configure load balancer or service mesh\n\
    \        return True\n\n    def _scale_up_service(self, incident: Incident, alert_data:\
    \ Dict) -> bool:\n        \"\"\"Scale up service experiencing high latency.\"\"\
    \"\n        # Implementation would use auto-scaling API\n        return True\n\
    \n    def assign_incident(self, incident_id: str, assignee: str):\n        \"\"\
    \"Assign incident to a person.\"\"\"\n        if incident_id in self.incidents:\n\
    \            self.incidents[incident_id].assigned_to = assignee\n            self.incidents[incident_id].updated_at\
    \ = datetime.utcnow()\n\n    def update_incident_status(self, incident_id: str,\
    \ status: IncidentStatus, notes: str = None):\n        \"\"\"Update incident status.\"\
    \"\"\n        if incident_id in self.incidents:\n            incident = self.incidents[incident_id]\n\
    \            incident.status = status\n            incident.updated_at = datetime.utcnow()\n\
    \            if notes:\n                incident.resolution_notes = notes\n\n\
    \    def _add_incident_note(self, incident_id: str, note: str):\n        \"\"\"\
    Add note to incident (simplified - would be in timeline in practice).\"\"\"\n\
    \        if incident_id in self.incidents:\n            incident = self.incidents[incident_id]\n\
    \            current_notes = incident.resolution_notes or \"\"\n            timestamp\
    \ = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')\n            incident.resolution_notes\
    \ = f\"{current_notes}\\n[{timestamp}] {note}\""
  Notification channel implementations would be separate classes: "class SlackNotifier:\n\
    \    def __init__(self, config): pass\n    def send_critical_alert(self, incident,\
    \ channel, alert_data): pass\n    def send_alert(self, incident, channel, alert_data):\
    \ pass\n    def create_channel(self, name, purpose, initial_members): pass\n \
    \   def pin_message(self, channel, message): pass\n\nclass PagerDutyNotifier:\n\
    \    def __init__(self, config): pass\n    def trigger_incident(self, incident_key,\
    \ description, details): pass\n\nclass EmailNotifier:\n    def __init__(self,\
    \ config): pass\n    def send_incident_notification(self, incident, recipients,\
    \ template): pass\n```\n\n---"
  Implementation Checklist: '### Observability Foundation

    - [ ] OpenTelemetry instrumentation implemented

    - [ ] Three pillars (metrics, logs, traces) configured

    - [ ] Service instrumentation automated

    - [ ] Context propagation working

    - [ ] Resource attributes standardized


    ### Metrics and Monitoring

    - [ ] Prometheus configured and running

    - [ ] Application metrics implemented

    - [ ] Recording rules defined

    - [ ] Business metrics tracked

    - [ ] Metric cardinality managed


    ### Distributed Tracing

    - [ ] Jaeger deployed and configured

    - [ ] Trace sampling configured

    - [ ] Advanced tracing patterns implemented

    - [ ] Trace analysis automated

    - [ ] Performance bottlenecks identified


    ### Logging

    - [ ] Structured logging implemented

    - [ ] Log aggregation configured (ELK/EFK)

    - [ ] Log retention policies defined

    - [ ] Correlation IDs implemented

    - [ ] Log analysis queries created


    ### SLOs and Error Budgets

    - [ ] SLOs defined for critical services

    - [ ] SLI queries implemented

    - [ ] Error budget tracking automated

    - [ ] SLO reporting dashboard created

    - [ ] Alerting on SLO violations configured


    ### Alerting and Incident Response

    - [ ] Alert rules comprehensive

    - [ ] AlertManager configured

    - [ ] Incident response automation implemented

    - [ ] On-call schedules configured

    - [ ] Runbooks created and linked


    ### Performance Monitoring

    - [ ] Performance baselines established

    - [ ] Anomaly detection configured

    - [ ] Capacity planning metrics tracked

    - [ ] Performance regression detection automated

    - [ ] Optimization recommendations automated


    ### Infrastructure Observability

    - [ ] Node exporter deployed

    - [ ] Kubernetes metrics collected

    - [ ] Container metrics monitored

    - [ ] Network metrics tracked

    - [ ] Storage metrics monitored


    ---


    **End of Observability and Monitoring Standards**'
metadata:
  version: 1.0.0
  last_updated: '2025-06-20T05:11:54.121590'
  source: williamzujkowski/standards/docs/standards/OBSERVABILITY_STANDARDS.md
  checksum: b9cbc302be79cb2d911242b6746c3021469e85a15e41e5d5bdb8b0ec86d43f65
