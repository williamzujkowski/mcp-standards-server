name: Docs/Standards/Testing Standards
category: development
filename: docs/standards/TESTING_STANDARDS.md
nist_controls: []
sections:
  Comprehensive Testing Manifesto for LLM Coding Projects: '**Version:** 1.0.0

    **Last Updated:** 2025-01-13

    **Status:** Active

    **Standard Code:** TS


    ---'
  Table of Contents: "1. [Core Testing Principles](#core-testing-principles)\n   -\
    \ [Hypothesis Tests for Behavior Validation](#1-hypothesis-tests-for-behavior-validation)\n\
    \   - [Regression Tests for Known Fail States](#2-regression-tests-for-known-fail-states)\n\
    \   - [Benchmark Tests with SLA Enforcement](#3-benchmark-tests-with-sla-enforcement)\n\
    \   - [Grammatical Evolution for Fuzzing](#4-grammatical-evolution-ge-for-fuzzing--edge-discovery)\n\
    \   - [Structured Logs for Agent Feedback](#5-structured-logs-for-agent-feedback)\n\
    2. [Quality Assurance Standards](#quality-assurance-standards)\n   - [Code Coverage\
    \ Requirements](#6-code-coverage-requirements)\n   - [Static Analysis Rules](#7-static-analysis-rules)\n\
    \   - [Contract Testing Framework](#8-contract-testing-framework)\n   - [Mutation\
    \ Testing Guidelines](#9-mutation-testing-guidelines)\n   - [Property-Based Testing\
    \ Framework](#10-property-based-testing-framework)\n3. [Security and Resilience](#security-and-resilience)\n\
    \   - [Security Testing Guidelines](#11-security-testing-guidelines)\n   - [Resilience\
    \ Testing Framework](#12-resilience-testing-framework)\n4. [Documentation and\
    \ Integration](#documentation-and-integration)\n   - [Documentation Testing](#13-documentation-testing)\n\
    \   - [Integration Testing Patterns](#14-integration-testing-patterns)\n   - [Testability\
    \ Guidelines](#15-testability-guidelines)"
  Overview: 'This standard provides comprehensive guidelines and best practices for
    the subject area.

    It aims to ensure consistency, quality, and maintainability across all related
    implementations.'
  Core Testing Principles: "<!-- @nist-controls: [si-10, si-11, au-2, au-3] -->\n\n\
    ### 1. Hypothesis Tests for Behavior Validation\n\n```\nWhen implementing a new\
    \ feature or function, create hypothesis tests that validate expected behaviors:\n\
    \n1. For each function, identify the core hypothesis of what it should accomplish\n\
    2. Write tests that:\n   - Define clear expectations (\"Given X, the function\
    \ should return Y\")\n   - Test both positive and negative cases\n   - Include\
    \ boundary conditions\n   - Verify error handling behaviors\n3. Express these\
    \ tests in the appropriate testing framework (e.g., pytest, Jest)\n4. Include\
    \ descriptive test names that document the behavior being validated\n\nExample\
    \ structure:\n```python\ndef test_user_authentication_valid_credentials():\n \
    \   \"\"\"HYPOTHESIS: Given valid credentials, authentication should succeed.\n\
    \    @nist ia-2 \"Test authentication mechanism\"\n    @nist si-10 \"Validate\
    \ input credentials\"\n    \"\"\"\n    # Arrange\n    valid_username = \"test_user\"\
    \n    valid_password = \"correct_password\"\n\n    # Act\n    result = authenticate_user(valid_username,\
    \ valid_password)\n\n    # Assert\n    assert result.success is True\n    assert\
    \ result.error_message is None\n```\n\n### 2. Regression Tests for Known Fail\
    \ States\n\n```\nWhen fixing bugs or addressing edge cases, always create regression\
    \ tests:\n\n1. For each bug fix, create a test that:\n   - Documents the original\
    \ issue clearly in the test description\n   - Recreates the exact conditions that\
    \ caused the failure\n   - Verifies the fix works as expected\n   - Includes issue/ticket\
    \ references for context\n2. Maintain a dedicated regression test suite that runs\
    \ with every build\n3. Label regression tests appropriately for traceability\n\
    4. Include timestamps and version information where relevant\n\nExample structure:\n\
    ```python\ndef test_calculation_with_zero_division_protection():\n    \"\"\"REGRESSION:\
    \ Bug #1234 - Division by zero crash in calculation module.\n    @nist si-11 \"\
    Test error handling\"\n    @nist si-10 \"Input validation testing\"\n\n    This\
    \ test ensures that when a divisor of zero is provided, the function\n    returns\
    \ a default value rather than raising an exception.\n    \"\"\"\n    # Arrange\n\
    \    input_value = 10\n    divisor = 0\n    expected_result = None  # Our fix\
    \ returns None instead of raising ZeroDivisionError\n\n    # Act\n    result =\
    \ safe_divide(input_value, divisor)\n\n    # Assert\n    assert result == expected_result\n\
    ```\n\n### 3. Benchmark Tests with SLA Enforcement\n\n```\nImplement benchmark\
    \ tests that enforce Service Level Agreements (SLAs):\n\n1. Define clear performance\
    \ metrics for your system:\n   - Response time / latency (milliseconds)\n   -\
    \ Throughput (requests per second)\n   - Resource usage (memory, CPU)\n   - Error\
    \ rates\n2. Create benchmark tests that:\n   - Establish baseline performance\
    \ expectations\n   - Run consistently in controlled environments\n   - Measure\
    \ against defined thresholds\n   - Alert on SLA violations\n3. Include both average\
    \ and percentile measurements (p95, p99)\n4. Document the testing environment\
    \ and conditions\n\nExample structure:\n```python\ndef test_api_response_time_sla():\n\
    \    \"\"\"BENCHMARK: API must respond within 200ms for 95% of requests.\n\n \
    \   SLA Requirements:\n    - p95 response time: < 200ms\n    - p99 response time:\
    \ < 500ms\n    - Error rate: < 0.1%\n    \"\"\"\n    # Arrange\n    num_requests\
    \ = 1000\n    endpoint = \"/api/users\"\n\n    # Act\n    response_times = []\n\
    \    errors = 0\n    for _ in range(num_requests):\n        start_time = time.time()\n\
    \        try:\n            response = client.get(endpoint)\n            if response.status_code\
    \ >= 400:\n                errors += 1\n        except Exception:\n          \
    \  errors += 1\n        finally:\n            response_times.append((time.time()\
    \ - start_time) * 1000)  # Convert to ms\n\n    # Assert\n    error_rate = errors\
    \ / num_requests\n    p95 = numpy.percentile(response_times, 95)\n    p99 = numpy.percentile(response_times,\
    \ 99)\n\n    assert p95 < 200, f\"95th percentile response time {p95}ms exceeds\
    \ SLA of 200ms\"\n    assert p99 < 500, f\"99th percentile response time {p99}ms\
    \ exceeds SLA of 500ms\"\n    assert error_rate < 0.001, f\"Error rate {error_rate}\
    \ exceeds SLA of 0.1%\"\n```\n\n### 4. Grammatical Evolution (GE) for Fuzzing\
    \ + Edge Discovery\n\n```\nImplement Grammatical Evolution (GE) for advanced fuzzing\
    \ and edge case discovery:\n\n1. Define a grammar that represents valid inputs\
    \ for your system:\n   - Create BNF (Backus-Naur Form) or similar grammar definition\n\
    \   - Include all possible input variations, formats, and structures\n   - Define\
    \ mutation operations that preserve grammatical correctness\n2. Implement an evolutionary\
    \ algorithm that:\n   - Generates test cases based on the grammar\n   - Evolves\
    \ test cases using fitness functions\n   - Prioritizes edge cases and unexpected\
    \ inputs\n   - Tracks code coverage to focus on unexplored paths\n3. Log and analyze\
    \ failures to identify patterns\n4. Automatically add discovered edge cases to\
    \ regression tests\n\nExample structure:\n```python\ndef test_with_grammatical_evolution():\n\
    \    \"\"\"FUZZING: Use GE to discover edge cases in the input parser.\n\n   \
    \ This test uses grammatical evolution to generate various inputs\n    that conform\
    \ to our API grammar but might trigger unexpected behaviors.\n    \"\"\"\n   \
    \ # Define grammar for API requests\n    grammar = {\n        'start': ['<request>'],\n\
    \        'request': ['{\"command\": \"<command>\", \"params\": <params>}'],\n\
    \        'command': ['get', 'set', 'update', 'delete', '<random_string>'],\n \
    \       'params': ['<simple_param>', '<complex_param>', '<nested_param>', '<malformed_param>'],\n\
    \        # ... additional grammar rules\n    }\n\n    # Configure GE parameters\n\
    \    max_generations = 50\n    population_size = 100\n    mutation_rate = 0.1\n\
    \n    # Run GE-based fuzzing\n    fuzzer = GrammaticalEvolutionFuzzer(grammar=grammar,\n\
    \                                      coverage_tracker=CoverageTracker(),\n \
    \                                     target_function=api_request_handler)\n\n\
    \    results = fuzzer.run(max_generations, population_size, mutation_rate)\n\n\
    \    # Analyze results\n    edge_cases = results.filter(lambda r: r.status ==\
    \ 'failure')\n\n    # Assert\n    assert not edge_cases.has_critical_failures(),\
    \ f\"Critical failures found: {edge_cases.critical_failures}\"\n\n    # Add discovered\
    \ edge cases to regression suite\n    for case in edge_cases:\n        add_to_regression_suite(case)\n\
    ```\n\n### 5. Structured Logs for Agent Feedback\n\n```\nImplement structured\
    \ logging for comprehensive agent feedback:\n\n1. Design a structured logging\
    \ system that captures:\n   - Input/output pairs for each agent operation\n  \
    \ - Decision points with considered alternatives\n   - Confidence scores for predictions\
    \ or responses\n   - Time and resource utilization metrics\n   - Any deviation\
    \ from expected behavior\n2. Use a consistent JSON or similar structured format\n\
    3. Include correlation IDs to track actions across system components\n4. Implement\
    \ log levels that enable filtering for different analysis needs\n5. Create analyzers\
    \ that process logs to identify patterns and issues\n\nExample structure:\n```python\n\
    def test_agent_logging_completeness():\n    \"\"\"AGENT FEEDBACK: Verify agent\
    \ produces comprehensive structured logs.\n\n    This test ensures our agent properly\
    \ logs all required information\n    for debugging, monitoring, and improvement\
    \ purposes.\n    \"\"\"\n    # Arrange\n    test_input = \"Process this complex\
    \ request with multiple steps\"\n    expected_log_fields = [\n        \"request_id\"\
    , \"timestamp\", \"input\", \"parsed_intent\",\n        \"selected_action\", \"\
    considered_alternatives\", \"confidence_score\",\n        \"execution_time_ms\"\
    , \"output\", \"status\"\n    ]\n\n    # Setup log capture\n    log_capture =\
    \ LogCapture()\n\n    # Act\n    agent.process(test_input, log_handler=log_capture)\n\
    \n    # Assert\n    logs = log_capture.get_logs_as_json()\n    assert len(logs)\
    \ > 0, \"No logs were produced\"\n\n    # Check if all required fields are present\
    \ in the logs\n    for log in logs:\n        for field in expected_log_fields:\n\
    \            assert field in log, f\"Required log field '{field}' is missing\"\
    \n\n    # Verify log sequence completeness\n    assert \"agent_started\" in [log[\"\
    event\"] for log in logs]\n    assert \"agent_completed\" in [log[\"event\"] for\
    \ log in logs]\n\n    # Verify decision points are logged with alternatives\n\
    \    decision_logs = [log for log in logs if log[\"event\"] == \"decision_point\"\
    ]\n    assert len(decision_logs) > 0, \"No decision points were logged\"\n   \
    \ for decision in decision_logs:\n        assert \"considered_alternatives\" in\
    \ decision\n        assert len(decision[\"considered_alternatives\"]) > 0\n```"
  Quality Assurance Standards: "### 6. Code Coverage Requirements\n\n```\nImplement\
    \ comprehensive code coverage standards in your testing:\n\n1. Establish minimum\
    \ code coverage thresholds:\n   - 85%+ overall line coverage\n   - 95%+ coverage\
    \ for critical components (authentication, data processing, API layers)\n   -\
    \ 100% coverage for utility functions and shared libraries\n\n2. Track coverage\
    \ trends over time:\n   - Prevent coverage regression in established code\n  \
    \ - Allow temporary exemptions for prototype code with documented expiration\n\
    \n3. Cover all code paths and branches:\n   - Test error handling and exception\
    \ paths\n   - Verify both positive and negative conditions for each branch\n \
    \  - Ensure conditional logic is fully exercised\n\n4. Include coverage reports\
    \ in CI/CD pipelines:\n   - Block merges that decrease coverage below thresholds\n\
    \   - Highlight uncovered code sections for reviewer attention\n\nExample implementation:\n\
    ```python"
  Coverage configuration (in pytest.ini, pyproject.toml, or similar): "[coverage]\n\
    fail_under = 85\nexclude_lines =\n    pragma: no cover\n    def __repr__\n   \
    \ raise NotImplementedError\n    if TYPE_CHECKING:\n    pass"
  Critical component coverage validation: "def test_coverage_critical_components():\n\
    \    \"\"\"Verify critical components meet 95% code coverage requirement.\"\"\"\
    \n    coverage_report = get_coverage_report()\n    critical_modules = [\n    \
    \    \"app/auth\",\n        \"app/data_processing\",\n        \"app/api\"\n  \
    \  ]\n\n    for module in critical_modules:\n        coverage = coverage_report.get_module_coverage(module)\n\
    \        assert coverage >= 95, f\"Critical module {module} has insufficient coverage:\
    \ {coverage}%\"\n```\n\n### 7. Static Analysis Rules\n\n```\nImplement static\
    \ analysis rules to catch issues before runtime:\n\n1. Configure linters and static\
    \ analyzers with appropriate rules:\n   - Enforce consistent code style (PEP 8,\
    \ ESLint, etc.)\n   - Find potential bugs (unused variables, unreachable code)\n\
    \   - Identify security vulnerabilities\n   - Detect performance anti-patterns\n\
    \n2. Create custom rules specific to your project:\n   - Domain-specific constraints\
    \ and conventions\n   - Architectural boundaries enforcement\n   - Resource management\
    \ patterns\n\n3. Integrate static analysis into development workflow:\n   - Run\
    \ checks before commits (pre-commit hooks)\n   - Include in CI/CD pipeline\n \
    \  - Generate reports for code reviews\n\n4. Maintain a \"zero warnings\" policy:\n\
    \   - Treat warnings as errors in CI builds\n   - Document temporary exceptions\
    \ with clear justification\n   - Schedule resolution of allowed exceptions\n\n\
    Example configuration:\n```python"
  Example pylint configuration: "[MASTER]\nignore=CVS,build,dist\npersistent=yes\n\
    \n[MESSAGES CONTROL]\ndisable=\n    missing-docstring,\n    invalid-name,\n  \
    \  too-few-public-methods\n\n[CUSTOM]"
  Project-specific rules: 'banned-modules=dangerous_module,insecure_library

    required-imports=app.telemetry,app.validation'
  Example pre-commit hook configuration: "repos:\n-   repo: https://github.com/pycqa/flake8\n\
    \    rev: 6.0.0\n    hooks:\n    -   id: flake8\n        additional_dependencies:\
    \ [\n            flake8-bandit,\n            flake8-bugbear,\n            flake8-docstrings,\n\
    \        ]\n```\n\n### 8. Contract Testing Framework\n\n```\nImplement contract\
    \ testing to verify interface stability between components:\n\n1. Define explicit\
    \ contracts for all service interfaces:\n   - Document expected request/response\
    \ formats\n   - Specify error handling behaviors\n   - Define performance characteristics\n\
    \   - Document version compatibility\n\n2. Implement consumer-driven contract\
    \ tests:\n   - Have consumers define their expectations of providers\n   - Verify\
    \ providers meet all consumer expectations\n   - Run tests on both consumer and\
    \ provider sides\n\n3. Maintain a contract registry:\n   - Track all service interface\
    \ contracts\n   - Version contracts explicitly\n   - Document migration paths\
    \ for breaking changes\n\n4. Automate contract verification:\n   - Test against\
    \ contract definitions, not implementations\n   - Include contract verification\
    \ in CI/CD pipelines\n   - Alert on contract violations\n\nExample implementation:\n\
    ```python\ndef test_user_service_contract():\n    \"\"\"Verify the User Service\
    \ meets its consumer contract requirements.\"\"\"\n    # Load contract definitions\n\
    \    contract = ContractRegistry.load(\"user_service\", version=\"2.1\")\n\n \
    \   # Test all required endpoints\n    for endpoint in contract.endpoints:\n \
    \       # Prepare test request based on contract\n        request = endpoint.create_sample_request()\n\
    \n        # Execute request against service\n        response = client.request(\n\
    \            method=endpoint.method,\n            url=endpoint.path,\n       \
    \     json=request.payload\n        )\n\n        # Verify response matches contract\n\
    \        assert response.status_code == endpoint.expected_status\n        assert\
    \ validate_schema(response.json(), endpoint.response_schema)\n\n        # Verify\
    \ performance requirements\n        assert response.elapsed.total_seconds() <\
    \ endpoint.sla_response_time\n```\n\n### 9. Mutation Testing Guidelines\n\n```\n\
    Implement mutation testing to verify test quality:\n\n1. Apply systematic code\
    \ mutations to verify test effectiveness:\n   - Replace conditional operators\
    \ (>, <, ==, etc.)\n   - Modify boundary values (+1, -1, etc.)\n   - Remove conditional\
    \ blocks\n   - Change logical operators (AND to OR, etc.)\n   - Swap function\
    \ calls with similar signatures\n\n2. Establish mutation score thresholds:\n \
    \  - Minimum 80% killed mutations overall\n   - 90%+ for critical components\n\
    \   - 100% for security-sensitive code\n\n3. Integrate into quality workflows:\n\
    \   - Run periodically (weekly/monthly) or on significant changes\n   - Review\
    \ surviving mutations in code reviews\n   - Address test gaps revealed by surviving\
    \ mutations\n\n4. Focus on high-value targets:\n   - Business logic\n   - Data\
    \ transformation code\n   - Security controls\n   - Error handling\n\nExample\
    \ implementation:\n```python"
  Configuration for mutation testing: '[mutmut]

    paths_to_mutate=src/

    backup=False

    runner=pytest

    tests_dir=tests/

    use_coverage_data=True'
  CI integration script: "def test_mutation_score():\n    \"\"\"Run mutation testing\
    \ and verify mutation score meets requirements.\"\"\"\n    # Run mutation testing\n\
    \    result = subprocess.run(\n        [\"mutmut\", \"run\", \"--ci\"],\n    \
    \    capture_output=True,\n        text=True\n    )\n\n    # Parse results\n \
    \   stats = parse_mutation_results(result.stdout)\n\n    # Verify overall score\n\
    \    assert stats[\"score\"] >= 80, f\"Mutation score too low: {stats['score']}%\"\
    \n\n    # Verify critical components\n    for component in CRITICAL_COMPONENTS:\n\
    \        component_score = stats[\"component_scores\"].get(component, 0)\n   \
    \     assert component_score >= 90, f\"Critical component {component} has insufficient\
    \ mutation score: {component_score}%\"\n```\n\n### 10. Property-Based Testing\
    \ Framework\n\n```\nImplement property-based testing to discover edge cases:\n\
    \n1. Define invariant properties your code must satisfy:\n   - Reversible operations\
    \ (encode/decode, serialize/deserialize)\n   - Mathematical properties (commutativity,\
    \ associativity)\n   - Business rule invariants\n   - Data transformation consistency\n\
    \n2. Generate diverse test inputs automatically:\n   - Configure generators for\
    \ your domain types\n   - Include edge cases and boundary values\n   - Combine\
    \ generators for complex structures\n   - Shrink failing cases to minimal examples\n\
    \n3. Define explicit property assertions:\n   - Focus on what must be true, not\
    \ specific examples\n   - Cover both functional and non-functional requirements\n\
    \   - Include performance and resource usage properties\n\n4. Incorporate failure\
    \ analysis:\n   - Document discovered edge cases\n   - Add specific regression\
    \ tests for found issues\n   - Refine generators based on findings\n\nExample\
    \ implementation:\n```python\nfrom hypothesis import given, strategies as st\n\
    \n@given(st.lists(st.integers()))\ndef test_sort_idempotence(values):\n    \"\"\
    \"PROPERTY: Sorting an already sorted list produces the same result.\"\"\"\n \
    \   once = sorted(values)\n    twice = sorted(once)\n    assert once == twice\n\
    \n@given(st.lists(st.integers()))\ndef test_sort_size_invariant(values):\n   \
    \ \"\"\"PROPERTY: Sorting preserves the size of the input list.\"\"\"\n    assert\
    \ len(sorted(values)) == len(values)\n\n@given(st.text())\ndef test_search_found_if_present(needle):\n\
    \    \"\"\"PROPERTY: If a string is in the text, search should find it.\"\"\"\n\
    \    # Skip empty strings\n    if not needle:\n        return\n\n    haystack\
    \ = f\"prefix {needle} suffix\"\n    assert search(haystack, needle) is not None\n\
    \n@given(st.dictionaries(st.text(), st.text()))\ndef test_serialization_roundtrip(data):\n\
    \    \"\"\"PROPERTY: Serialize+deserialize should return the original data.\"\"\
    \"\n    serialized = serialize_to_json(data)\n    deserialized = deserialize_from_json(serialized)\n\
    \    assert deserialized == data\n```"
  Security and Resilience: "### 11. Security Testing Guidelines\n\n<!-- @nist-controls:\
    \ [si-10, si-11, ac-3, ac-6, ia-2, sc-8, sc-13, au-2] -->\n\n```\nImplement comprehensive\
    \ security testing practices:\n\n1. Apply security testing at multiple levels:\n\
    \   - Static analysis for known vulnerable patterns\n   - Dependency scanning\
    \ for known vulnerabilities\n   - Dynamic testing (DAST) for runtime vulnerabilities\n\
    \   - Interactive testing (IAST) for complex attack vectors\n\n2. Test against\
    \ established security standards:\n   - OWASP Top 10 vulnerabilities\n   - CWE/SANS\
    \ Top 25 programming errors\n   - Domain-specific security requirements (HIPAA,\
    \ PCI-DSS, etc.)\n\n3. Implement specific test cases for common vulnerabilities:\n\
    \   - Injection attacks (SQL, NoSQL, OS command, etc.)\n   - Authentication and\
    \ authorization bypass\n   - Sensitive data exposure\n   - XXE and SSRF vulnerabilities\n\
    \   - Security misconfigurations\n\n4. Incorporate security tests into CI/CD:\n\
    \   - Block deployment on critical findings\n   - Generate security reports for\
    \ review\n   - Track security debt and remediation\n\nExample implementation:\n\
    ```python\ndef test_sql_injection_prevention():\n    \"\"\"Verify protection against\
    \ SQL injection attacks.\n    @nist si-10 \"Input validation testing\"\n    @evidence\
    \ test\n    \"\"\"\n    # Arrange\n    attack_vectors = [\n        \"' OR '1'='1\"\
    ,\n        \"'; DROP TABLE users; --\",\n        \"' UNION SELECT username, password\
    \ FROM users; --\",\n        # More attack vectors...\n    ]\n\n    # Act & Assert\n\
    \    for attack in attack_vectors:\n        # Test each input point that might\
    \ reach database\n        result = user_service.find_by_name(attack)\n       \
    \ assert not any_user_data_leaked(result)\n\n        result = user_service.authenticate(attack,\
    \ \"password\")\n        assert result.authenticated is False\n\ndef test_authorization_controls():\n\
    \    \"\"\"Verify proper enforcement of authorization controls.\n    @nist ac-3\
    \ \"Access enforcement testing\"\n    @nist ac-6 \"Least privilege verification\"\
    \n    @evidence test\n    \"\"\"\n    # Arrange\n    regular_user = create_user(role=\"\
    user\")\n    admin_user = create_user(role=\"admin\")\n\n    # Act & Assert\n\
    \    # Test vertical privilege escalation\n    regular_user_client = get_client_for_user(regular_user)\n\
    \    response = regular_user_client.get(\"/admin/users\")\n    assert response.status_code\
    \ == 403\n\n    # Test horizontal privilege escalation\n    other_user = create_user(role=\"\
    user\")\n    response = regular_user_client.get(f\"/users/{other_user.id}/profile\"\
    )\n    assert response.status_code == 403\n```\n\n### 12. Resilience Testing Framework\n\
    \n```\nImplement resilience testing to verify system stability under adverse conditions:\n\
    \n1. Design chaos engineering experiments:\n   - Service/dependency failures\n\
    \   - Network degradation/partitioning\n   - Resource exhaustion (CPU, memory,\
    \ disk)\n   - Clock skew and time-related issues\n   - High latency and throughput\
    \ scenarios\n\n2. Define steady-state hypotheses for each experiment:\n   - Explicit\
    \ metrics that define normal operation\n   - Acceptable degradation limits\n \
    \  - Recovery time objectives\n\n3. Run controlled experiments in production-like\
    \ environments:\n   - Start with minimal blast radius\n   - Increase scope progressively\n\
    \   - Always have abort conditions\n   - Monitor closely during execution\n\n\
    4. Incorporate findings into architecture:\n   - Document discovered weaknesses\n\
    \   - Implement additional safeguards\n   - Create regression tests for found\
    \ issues\n\nExample implementation:\n```python\ndef test_resilience_to_database_failure():\n\
    \    \"\"\"Verify system resilience when database becomes unavailable.\n    @nist\
    \ si-11 \"Error handling under failure conditions\"\n    @evidence test\n    \"\
    \"\"\n    # Define steady state\n    def check_steady_state():\n        response\
    \ = client.get(\"/api/health\")\n        return (\n            response.status_code\
    \ == 200 and\n            response.json()[\"status\"] == \"healthy\" and\n   \
    \         response.elapsed.total_seconds() < 0.5\n        )\n\n    # Verify initial\
    \ steady state\n    assert check_steady_state()\n\n    # Introduce chaos - database\
    \ failure\n    with simulate_database_failure():\n        # Verify degraded but\
    \ operational state\n        response = client.get(\"/api/users\")\n\n       \
    \ # Read operations should work from cache\n        assert response.status_code\
    \ == 200\n        assert response.headers[\"X-Data-Source\"] == \"cache\"\n\n\
    \        # Write operations should fail gracefully\n        create_response =\
    \ client.post(\"/api/users\", json={\"name\": \"test\"})\n        assert create_response.status_code\
    \ == 503  # Service Unavailable\n        assert \"retry_after\" in create_response.headers\n\
    \n    # Verify recovery\n    # Wait for recovery - max 30 seconds\n    wait_for_condition(check_steady_state,\
    \ timeout=30)\n\n    # Verify writes work after recovery\n    create_response\
    \ = client.post(\"/api/users\", json={\"name\": \"test\"})\n    assert create_response.status_code\
    \ == 201  # Created\n```"
  Documentation and Integration: "### 13. Documentation Testing\n\n```\nImplement\
    \ documentation testing to ensure accuracy and reliability:\n\n1. Test all code\
    \ examples in documentation:\n   - Extract code blocks from documentation\n  \
    \ - Execute in isolated environments\n   - Verify expected outputs match documented\
    \ claims\n   - Update examples when APIs change\n\n2. Validate API documentation\
    \ completeness:\n   - Test that all endpoints are documented\n   - Verify all\
    \ parameters are described\n   - Ensure all response codes are documented\n  \
    \ - Check that examples cover common use cases\n\n3. Test documentation for user\
    \ journeys:\n   - Verify step-by-step tutorials work as written\n   - Test installation\
    \ and setup instructions\n   - Validate troubleshooting guides\n\n4. Automate\
    \ documentation testing:\n   - Run doc tests in CI/CD pipelines\n   - Generate\
    \ documentation from tests\n   - Flag documentation drift\n\nExample implementation:\n\
    ```python\ndef test_readme_examples():\n    \"\"\"Verify all code examples in\
    \ README.md work as documented.\"\"\"\n    # Extract code examples from README\n\
    \    readme_path = Path(\"README.md\")\n    readme_content = readme_path.read_text()\n\
    \    code_blocks = extract_code_blocks(readme_content)\n\n    for i, code in enumerate(code_blocks):\n\
    \        # Skip non-executable examples (e.g., console output)\n        if is_executable_code(code):\n\
    \            # Create temporary module for execution\n            module_path\
    \ = tmp_path / f\"example_{i}.py\"\n            module_path.write_text(code)\n\
    \n            # Execute example\n            result = subprocess.run(\n      \
    \          [sys.executable, str(module_path)],\n                capture_output=True,\n\
    \                text=True\n            )\n\n            # Verify execution succeeds\n\
    \            assert result.returncode == 0, f\"Example {i} failed: {result.stderr}\"\
    \n\n            # Verify output matches if specified\n            expected_output\
    \ = extract_expected_output(readme_content, i)\n            if expected_output:\n\
    \                assert expected_output in result.stdout\n\ndef test_api_documentation_completeness():\n\
    \    \"\"\"Verify API documentation covers all actual endpoints.\"\"\"\n    #\
    \ Get actual API endpoints\n    actual_endpoints = list_all_api_endpoints()\n\n\
    \    # Get documented endpoints\n    documented_endpoints = extract_endpoints_from_docs()\n\
    \n    # Verify all actual endpoints are documented\n    for endpoint in actual_endpoints:\n\
    \        assert endpoint in documented_endpoints, f\"Endpoint {endpoint} is not\
    \ documented\"\n\n        # Verify all parameters are documented\n        actual_params\
    \ = get_endpoint_parameters(endpoint)\n        documented_params = get_documented_parameters(endpoint)\n\
    \        assert set(actual_params) == set(documented_params), \\\n           \
    \ f\"Parameter mismatch for {endpoint}: actual {actual_params}, documented {documented_params}\"\
    \n```\n\n### 14. Integration Testing Patterns\n\n```\nImplement robust integration\
    \ testing patterns:\n\n1. Define integration boundaries explicitly:\n   - Component\
    \ interactions\n   - Service dependencies\n   - Third-party integrations\n   -\
    \ Database and persistence layers\n\n2. Use appropriate testing approaches for\
    \ each boundary:\n   - Mocks for uncontrollable dependencies\n   - Stubs for simplified\
    \ behavior\n   - Spies for interaction verification\n   - Real instances for critical\
    \ paths\n\n3. Implement end-to-end test scenarios:\n   - Complete user journeys\n\
    \   - Multi-step business processes\n   - Cross-component workflows\n\n4. Manage\
    \ test environments effectively:\n   - Containerize dependencies\n   - Use test\
    \ doubles when appropriate\n   - Reset state between tests\n   - Parallel testing\
    \ support\n\nExample implementation:\n```python\ndef test_user_registration_end_to_end():\n\
    \    \"\"\"Verify the complete user registration process across all components.\"\
    \"\"\n    # Arrange\n    email = f\"test-{uuid.uuid4()}@example.com\"\n    password\
    \ = \"secure_password123\"\n\n    # Use real email service in test mode\n    email_service\
    \ = configure_real_email_service(test_mode=True)\n\n    # Act - Start registration\
    \ process\n    response = client.post(\"/api/register\", json={\n        \"email\"\
    : email,\n        \"password\": password\n    })\n\n    # Assert - Initial response\n\
    \    assert response.status_code == 202  # Accepted\n    registration_id = response.json()[\"\
    registration_id\"]\n\n    # Verify email was sent\n    sent_emails = email_service.get_sent_emails()\n\
    \    assert len(sent_emails) == 1\n    verification_email = sent_emails[0]\n \
    \   assert verification_email.recipient == email\n\n    # Extract verification\
    \ code\n    verification_code = extract_verification_code(verification_email.body)\n\
    \n    # Complete verification\n    response = client.post(\"/api/verify\", json={\n\
    \        \"registration_id\": registration_id,\n        \"verification_code\"\
    : verification_code\n    })\n    assert response.status_code == 201  # Created\n\
    \n    # Verify user is created in database\n    with db_connection() as conn:\n\
    \        user = conn.execute(\n            \"SELECT * FROM users WHERE email =\
    \ ?\",\n            (email,)\n        ).fetchone()\n        assert user is not\
    \ None\n        assert user[\"email\"] == email\n        assert user[\"is_verified\"\
    ] is True\n\n    # Verify login works\n    response = client.post(\"/api/login\"\
    , json={\n        \"email\": email,\n        \"password\": password\n    })\n\
    \    assert response.status_code == 200\n    assert \"auth_token\" in response.json()\n\
    ```\n\n### 15. Testability Guidelines\n\n```\nImplement testability guidelines\
    \ to ensure code is designed for effective testing:\n\n1. Design for testability:\n\
    \   - Use dependency injection\n   - Separate concerns clearly\n   - Avoid global\
    \ state\n   - Create pure functions where possible\n\n2. Create testability interfaces:\n\
    \   - Time providers instead of direct datetime usage\n   - File system abstractions\n\
    \   - Network/API abstractions\n   - Randomness control\n\n3. Implement testing\
    \ hooks:\n   - Instrumentation points\n   - State inspection methods\n   - Execution\
    \ trace capabilities\n   - Test-only extension points\n\n4. Establish testability\
    \ reviews:\n   - Include testability in code reviews\n   - Measure test effort\
    \ as metric\n   - Refactor hard-to-test code\n\nExample implementation:\n```python"
  'Instead of this:': "class HardToTestService:\n    def process_data(self, input_data):\n\
    \        current_time = datetime.now()\n        processed = self._transform(input_data)\n\
    \        if random.random() < 0.5:\n            return self._special_case(processed,\
    \ current_time)\n        return processed"
  'Do this:': "class TestableService:\n    def __init__(self, time_provider=None,\
    \ random_provider=None):\n        self.time_provider = time_provider or (lambda:\
    \ datetime.now())\n        self.random_provider = random_provider or random.random\n\
    \n    def process_data(self, input_data):\n        current_time = self.time_provider()\n\
    \        processed = self._transform(input_data)\n        if self.random_provider()\
    \ < 0.5:\n            return self._special_case(processed, current_time)\n   \
    \     return processed"
  'Now testing becomes straightforward:': "def test_process_data_normal_path():\n\
    \    \"\"\"Test the normal processing path.\"\"\"\n    # Arrange\n    service\
    \ = TestableService(\n        time_provider=lambda: datetime(2023, 1, 1, 12, 0,\
    \ 0),\n        random_provider=lambda: 0.6  # Ensures we skip the special case\n\
    \    )\n    input_data = {\"key\": \"value\"}\n\n    # Act\n    result = service.process_data(input_data)\n\
    \n    # Assert\n    assert result == expected_transformed_data\n\ndef test_process_data_special_case():\n\
    \    \"\"\"Test the special case processing path.\"\"\"\n    # Arrange\n    fixed_time\
    \ = datetime(2023, 1, 1, 12, 0, 0)\n    service = TestableService(\n        time_provider=lambda:\
    \ fixed_time,\n        random_provider=lambda: 0.4  # Ensures we take the special\
    \ case\n    )\n    input_data = {\"key\": \"value\"}\n\n    # Act\n    result\
    \ = service.process_data(input_data)\n\n    # Assert\n    assert result == expected_special_case_result\n\
    ```"
  Master Prompt for Test Suite Generation: "```\nGenerate a comprehensive test suite\
    \ for this code that follows the Complete Testing Manifesto:\n\n1. Core Testing\
    \ Principles:\n   - Hypothesis tests that validate core behaviors and expectations\n\
    \   - Regression tests that prevent known bugs from reappearing\n   - Benchmark\
    \ tests that enforce performance SLAs\n   - Grammatical Evolution for fuzzing\
    \ and edge case discovery\n   - Structured logs for agent feedback and analysis\n\
    \n2. Quality Assurance:\n   - Code coverage meeting standards (85%+ overall, 95%+\
    \ for critical paths)\n   - Static analysis rules to catch potential issues early\n\
    \   - Contract tests for interface stability between components\n   - Mutation\
    \ testing to verify test suite effectiveness\n   - Property-based testing to validate\
    \ code invariants\n\n3. Security and Resilience:\n   - Security tests for common\
    \ vulnerabilities and attacks\n   - Resilience tests that verify system behavior\
    \ under adverse conditions\n\n4. Documentation and Integration:\n   - Documentation\
    \ tests to ensure examples and guides remain accurate\n   - Integration tests\
    \ for key component interactions and user journeys\n   - Testability improvements\
    \ to make the code more easily verifiable\n\nFor each test:\n- Include clear documentation\
    \ of purpose and expected behavior\n- Provide detailed setup and context for test\
    \ conditions\n- Write explicit assertions with descriptive failure messages\n\
    - Categorize appropriately (unit, integration, security, etc.)\n\nThe test suite\
    \ should be maintainable, provide fast feedback, and serve as living documentation\
    \ of the system's behavior and constraints."
  Implementation: '### Getting Started


    1. Review the relevant sections of this standard for your use case

    2. Identify which guidelines apply to your project

    3. Implement the required practices and patterns

    4. Validate compliance using the provided checklists


    ### Implementation Checklist


    - [ ] Review and understand applicable standards

    - [ ] Implement required practices

    - [ ] Follow recommended patterns

    - [ ] Validate implementation against guidelines

    - [ ] Document any deviations with justification'
  Related Standards: '- [Knowledge Management Standards](KNOWLEDGE_MANAGEMENT_STANDARDS.md)
    - Documentation practices

    - [CREATING_STANDARDS_GUIDE.md](./docs/guides/CREATING_STANDARDS_GUIDE.md) - Standards
    creation guide

    - [COMPLIANCE_STANDARDS.md](COMPLIANCE_STANDARDS.md) - NIST compliance testing
    requirements'
metadata:
  version: 1.0.0
  last_updated: '2025-06-20T05:11:53.234042'
  source: williamzujkowski/standards/docs/standards/TESTING_STANDARDS.md
  checksum: 688a709e9a883c0f1cdd0f3e622589af0252020e128be67422355b9d6a489e32
