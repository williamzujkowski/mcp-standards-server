name: Docs/Standards/Devops Platform Standards
category: general
filename: docs/standards/DEVOPS_PLATFORM_STANDARDS.md
nist_controls: []
sections:
  DevOps and Platform Engineering Standards: '**Version:** 1.0.0

    **Last Updated:** January 2025

    **Status:** Active

    **Standard Code:** DOP


    ---


    **Version:** 1.0.0

    **Last Updated:** January 2025

    **Status:** Active'
  Table of Contents: '1. [Infrastructure as Code (IaC)](#1-infrastructure-as-code-iac)

    2. [CI/CD Pipeline Standards](#2-cicd-pipeline-standards)

    3. [Container Orchestration](#3-container-orchestration)

    4. [Platform Engineering](#4-platform-engineering)

    5. [Site Reliability Engineering](#5-site-reliability-engineering)

    6. [GitOps and Deployment](#6-gitops-and-deployment)

    7. [Configuration Management](#7-configuration-management)

    8. [Release Management](#8-release-management)


    ---'
  Overview: 'This standard provides comprehensive guidelines and best practices for
    the subject area.

    It aims to ensure consistency, quality, and maintainability across all related
    implementations.'
  1. Infrastructure as Code (IaC): '### 1.1 Terraform Standards


    #### Core Terraform Practices **[REQUIRED]**

    ```hcl'
  terraform/modules/example/main.tf: "terraform {\n  required_version = \">= 1.5.0\"\
    \n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n \
    \     version = \"~> 5.0\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket     \
    \    = \"terraform-state-bucket\"\n    key            = \"infrastructure/terraform.tfstate\"\
    \n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table\
    \ = \"terraform-state-lock\"\n  }\n}"
  Module structure: "module \"network\" {\n  source = \"./modules/network\"\n\n  environment\
    \ = var.environment\n  cidr_block  = var.vpc_cidr\n\n  tags = merge(\n    local.common_tags,\n\
    \    {\n      Module = \"network\"\n    }\n  )\n}\n```\n\n#### State Management\
    \ **[REQUIRED]**\n```yaml"
  State management configuration: "state_management:\n  backend:\n    type: \"s3\"\
    \  # or \"azurerm\", \"gcs\"\n    encryption: true\n    locking: true\n    versioning:\
    \ true\n\n  workspace_strategy:\n    pattern: \"{environment}-{region}\"\n   \
    \ isolation: \"complete\"\n\n  security:\n    access_control: \"role-based\"\n\
    \    audit_logging: true\n    state_encryption: \"at-rest\"\n```\n\n#### Module\
    \ Design **[REQUIRED]**\n```hcl"
  modules/rds/variables.tf: "variable \"instance_class\" {\n  description = \"RDS\
    \ instance class\"\n  type        = string\n\n  validation {\n    condition  \
    \   = can(regex(\"^db\\\\.\", var.instance_class))\n    error_message = \"Instance\
    \ class must start with 'db.'\"\n  }\n}\n\nvariable \"allocated_storage\" {\n\
    \  description = \"Allocated storage in GB\"\n  type        = number\n  default\
    \     = 100\n\n  validation {\n    condition     = var.allocated_storage >= 20\
    \ && var.allocated_storage <= 65536\n    error_message = \"Storage must be between\
    \ 20 and 65536 GB.\"\n  }\n}"
  modules/rds/outputs.tf: "output \"endpoint\" {\n  description = \"RDS instance endpoint\"\
    \n  value       = aws_db_instance.main.endpoint\n  sensitive   = false\n}\n\n\
    output \"connection_string\" {\n  description = \"Database connection string\"\
    \n  value       = \"postgresql://${aws_db_instance.main.username}@${aws_db_instance.main.endpoint}/${aws_db_instance.main.db_name}\"\
    \n  sensitive   = true\n}\n```\n\n### 1.2 Ansible Automation\n\n#### Playbook\
    \ Standards **[REQUIRED]**\n```yaml"
  ansible/playbooks/deploy-app.yml: "---\n- name: Deploy Application\n  hosts: app_servers\n\
    \  become: yes\n  gather_facts: yes\n\n  vars_files:\n    - ../vars/{{ environment\
    \ }}.yml\n    - ../vars/secrets.yml\n\n  pre_tasks:\n    - name: Verify target\
    \ environment\n      assert:\n        that:\n          - environment is defined\n\
    \          - environment in ['dev', 'staging', 'prod']\n        fail_msg: \"Environment\
    \ must be specified and valid\"\n\n    - name: Create deployment audit log\n \
    \     lineinfile:\n        path: /var/log/deployments.log\n        line: \"{{\
    \ ansible_date_time.iso8601 }} - {{ ansible_user }} - {{ app_version }}\"\n  \
    \      create: yes\n\n  roles:\n    - role: common\n      tags: ['always']\n \
    \   - role: app-deploy\n      tags: ['deploy']\n    - role: health-check\n   \
    \   tags: ['verify']\n\n  post_tasks:\n    - name: Send deployment notification\n\
    \      uri:\n        url: \"{{ slack_webhook_url }}\"\n        method: POST\n\
    \        body_format: json\n        body:\n          text: \"Deployment complete:\
    \ {{ app_name }} v{{ app_version }} to {{ environment }}\"\n      when: notify_slack\
    \ | default(true)\n```\n\n#### Role Structure **[REQUIRED]**\n```yaml"
  ansible/roles/app-deploy/tasks/main.yml: "---\n- name: Include OS-specific variables\n\
    \  include_vars: \"{{ ansible_os_family }}.yml\"\n\n- name: Validate deployment\
    \ prerequisites\n  include_tasks: validate.yml\n  tags: ['validate']\n\n- name:\
    \ Prepare deployment directory\n  file:\n    path: \"{{ app_directory }}\"\n \
    \   state: directory\n    owner: \"{{ app_user }}\"\n    group: \"{{ app_group\
    \ }}\"\n    mode: '0755'\n\n- name: Deploy application artifacts\n  unarchive:\n\
    \    src: \"{{ artifact_url }}\"\n    dest: \"{{ app_directory }}\"\n    remote_src:\
    \ yes\n    owner: \"{{ app_user }}\"\n    group: \"{{ app_group }}\"\n    validate_certs:\
    \ yes\n  notify:\n    - restart application\n    - verify application health\n\
    \n- name: Template configuration files\n  template:\n    src: \"{{ item.src }}\"\
    \n    dest: \"{{ item.dest }}\"\n    owner: \"{{ app_user }}\"\n    group: \"\
    {{ app_group }}\"\n    mode: \"{{ item.mode | default('0644') }}\"\n    backup:\
    \ yes\n  loop:\n    - { src: 'app.conf.j2', dest: '/etc/app/app.conf' }\n    -\
    \ { src: 'env.j2', dest: '{{ app_directory }}/.env', mode: '0600' }\n  notify:\n\
    \    - reload configuration\n```\n\n### 1.3 Infrastructure Testing\n\n#### Terraform\
    \ Testing **[REQUIRED]**\n```hcl"
  tests/terraform/network_test.go: "package test\n\nimport (\n    \"testing\"\n  \
    \  \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\
    \n)\n\nfunc TestNetworkModule(t *testing.T) {\n    t.Parallel()\n\n    terraformOptions\
    \ := &terraform.Options{\n        TerraformDir: \"../../modules/network\",\n \
    \       Vars: map[string]interface{}{\n            \"environment\": \"test\",\n\
    \            \"cidr_block\":  \"10.0.0.0/16\",\n        },\n    }\n\n    defer\
    \ terraform.Destroy(t, terraformOptions)\n    terraform.InitAndApply(t, terraformOptions)\n\
    \n    // Validate outputs\n    vpcId := terraform.Output(t, terraformOptions,\
    \ \"vpc_id\")\n    assert.NotEmpty(t, vpcId)\n\n    // Test network connectivity\n\
    \    validateNetworkConnectivity(t, terraformOptions)\n}\n```\n\n#### Ansible\
    \ Testing **[RECOMMENDED]**\n```yaml"
  molecule/default/molecule.yml: "---\ndependency:\n  name: galaxy\ndriver:\n  name:\
    \ docker\nplatforms:\n  - name: instance\n    image: \"geerlingguy/docker-${MOLECULE_DISTRO:-ubuntu2004}-ansible:latest\"\
    \n    command: ${MOLECULE_DOCKER_COMMAND:-\"\"}\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\n\
    \    privileged: true\n    pre_build_image: true\nprovisioner:\n  name: ansible\n\
    \  playbooks:\n    converge: ${MOLECULE_PLAYBOOK:-converge.yml}\nverifier:\n \
    \ name: ansible\nlint: |\n  set -e\n  yamllint .\n  ansible-lint .\n```\n\n---"
  2. CI/CD Pipeline Standards: '### 2.1 Pipeline Architecture


    #### Pipeline Stages **[REQUIRED]**

    ```yaml'
  .github/workflows/ci-cd.yml: "name: CI/CD Pipeline\n\non:\n  push:\n    branches:\
    \ [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  DOCKER_BUILDKIT:\
    \ 1\n  COMPOSE_DOCKER_CLI_BUILD: 1\n\njobs:\n  # 1. Code Quality Stage\n  quality:\n\
    \    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n \
    \       with:\n          fetch-depth: 0  # Full history for better analysis\n\n\
    \      - name: Code Quality Checks\n        run: |\n          make lint\n    \
    \      make format-check\n          make type-check\n\n      - name: Security\
    \ Scanning\n        uses: aquasecurity/trivy-action@master\n        with:\n  \
    \        scan-type: 'fs'\n          scan-ref: '.'\n          severity: 'CRITICAL,HIGH'\n\
    \n      - name: License Compliance\n        run: |\n          pip install pip-licenses\n\
    \          pip-licenses --fail-on=\"GPL\"\n\n  # 2. Build Stage\n  build:\n  \
    \  needs: quality\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n\
    \        target: [app, worker]\n    steps:\n      - uses: actions/checkout@v4\n\
    \n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\
    \n      - name: Build and Cache\n        uses: docker/build-push-action@v5\n \
    \       with:\n          context: .\n          file: ./docker/Dockerfile.${{ matrix.target\
    \ }}\n          push: false\n          tags: app:${{ matrix.target }}-${{ github.sha\
    \ }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n\
    \          outputs: type=docker,dest=/tmp/${{ matrix.target }}.tar\n\n      -\
    \ name: Upload artifact\n        uses: actions/upload-artifact@v3\n        with:\n\
    \          name: ${{ matrix.target }}-image\n          path: /tmp/${{ matrix.target\
    \ }}.tar\n          retention-days: 1\n\n  # 3. Test Stage\n  test:\n    needs:\
    \ build\n    runs-on: ubuntu-latest\n    services:\n      postgres:\n        image:\
    \ postgres:15\n        env:\n          POSTGRES_PASSWORD: test\n        options:\
    \ >-\n          --health-cmd pg_isready\n          --health-interval 10s\n   \
    \       --health-timeout 5s\n          --health-retries 5\n    steps:\n      -\
    \ uses: actions/checkout@v4\n\n      - name: Download artifacts\n        uses:\
    \ actions/download-artifact@v3\n\n      - name: Load Docker images\n        run:\
    \ |\n          docker load --input app-image/app.tar\n          docker load --input\
    \ worker-image/worker.tar\n\n      - name: Run Tests\n        run: |\n       \
    \   make test-unit\n          make test-integration\n          make test-e2e\n\
    \        env:\n          DATABASE_URL: postgresql://postgres:test@localhost:5432/test\n\
    \n      - name: Upload Coverage\n        uses: codecov/codecov-action@v3\n   \
    \     with:\n          fail_ci_if_error: true\n\n  # 4. Deploy Stage\n  deploy:\n\
    \    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\
    \    steps:\n      - uses: actions/checkout@v4\n\n      - name: Configure AWS\
    \ credentials\n        uses: aws-actions/configure-aws-credentials@v4\n      \
    \  with:\n          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE }}\n         \
    \ aws-region: us-east-1\n\n      - name: Deploy to ECS\n        run: |\n     \
    \     make deploy-ecs ENV=production\n```\n\n#### Jenkins Pipeline **[ALTERNATIVE]**\n\
    ```groovy\n// Jenkinsfile\n@Library('shared-pipeline-library@v2') _\n\npipeline\
    \ {\n    agent {\n        kubernetes {\n            yamlFile 'jenkins/pod-templates/build-pod.yaml'\n\
    \        }\n    }\n\n    options {\n        timeout(time: 1, unit: 'HOURS')\n\
    \        timestamps()\n        ansiColor('xterm')\n        buildDiscarder(logRotator(numToKeepStr:\
    \ '30'))\n    }\n\n    environment {\n        DOCKER_REGISTRY = credentials('docker-registry')\n\
    \        SONAR_TOKEN = credentials('sonar-token')\n    }\n\n    stages {\n   \
    \     stage('Checkout') {\n            steps {\n                checkout scm\n\
    \                script {\n                    env.GIT_COMMIT_SHORT = sh(\n  \
    \                      script: \"git rev-parse --short HEAD\",\n             \
    \           returnStdout: true\n                    ).trim()\n               \
    \ }\n            }\n        }\n\n        stage('Quality Gates') {\n          \
    \  parallel {\n                stage('Lint') {\n                    steps {\n\
    \                        sh 'make lint'\n                    }\n             \
    \   }\n                stage('Security Scan') {\n                    steps {\n\
    \                        sh 'make security-scan'\n                    }\n    \
    \            }\n                stage('SonarQube') {\n                    steps\
    \ {\n                        withSonarQubeEnv('SonarQube') {\n               \
    \             sh 'make sonar-scan'\n                        }\n              \
    \      }\n                }\n            }\n        }\n\n        stage('Build')\
    \ {\n            steps {\n                script {\n                    docker.withRegistry(DOCKER_REGISTRY)\
    \ {\n                        def app = docker.build(\"app:${GIT_COMMIT_SHORT}\"\
    )\n                        app.push()\n                        app.push('latest')\n\
    \                    }\n                }\n            }\n        }\n\n      \
    \  stage('Test') {\n            steps {\n                sh 'make test-all'\n\
    \                junit 'reports/**/*.xml'\n                publishHTML([\n   \
    \                 allowMissing: false,\n                    alwaysLinkToLastBuild:\
    \ true,\n                    keepAll: true,\n                    reportDir: 'coverage',\n\
    \                    reportFiles: 'index.html',\n                    reportName:\
    \ 'Coverage Report'\n                ])\n            }\n        }\n\n        stage('Deploy')\
    \ {\n            when {\n                branch 'main'\n            }\n      \
    \      steps {\n                script {\n                    deployToKubernetes(\n\
    \                        namespace: 'production',\n                        deployment:\
    \ 'app',\n                        image: \"app:${GIT_COMMIT_SHORT}\"\n       \
    \             )\n                }\n            }\n        }\n    }\n\n    post\
    \ {\n        always {\n            cleanWs()\n        }\n        success {\n \
    \           slackSend(\n                color: 'good',\n                message:\
    \ \"Build Successful: ${env.JOB_NAME} - ${env.BUILD_NUMBER}\"\n            )\n\
    \        }\n        failure {\n            slackSend(\n                color:\
    \ 'danger',\n                message: \"Build Failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}\"\
    \n            )\n        }\n    }\n}\n```\n\n### 2.2 Deployment Strategies\n\n\
    #### Blue-Green Deployment **[RECOMMENDED]**\n```yaml"
  kubernetes/deployments/blue-green.yaml: "apiVersion: v1\nkind: Service\nmetadata:\n\
    \  name: app-service\nspec:\n  selector:\n    app: myapp\n    version: green \
    \ # Switch between blue/green\n  ports:\n    - port: 80\n      targetPort: 8080\n\
    \n---"
  Blue Deployment: "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-blue\n\
    spec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n      version:\
    \ blue\n  template:\n    metadata:\n      labels:\n        app: myapp\n      \
    \  version: blue\n    spec:\n      containers:\n      - name: app\n        image:\
    \ myapp:1.0.0\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n\
    \          httpGet:\n            path: /health\n            port: 8080\n     \
    \     initialDelaySeconds: 10\n          periodSeconds: 5\n\n---"
  Green Deployment (New Version): "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n\
    \  name: app-green\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n    \
    \  app: myapp\n      version: green\n  template:\n    metadata:\n      labels:\n\
    \        app: myapp\n        version: green\n    spec:\n      containers:\n  \
    \    - name: app\n        image: myapp:2.0.0\n        ports:\n        - containerPort:\
    \ 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n\
    \            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds:\
    \ 5\n```\n\n#### Canary Deployment **[RECOMMENDED]**\n```yaml"
  kubernetes/deployments/canary.yaml: "apiVersion: flagger.app/v1beta1\nkind: Canary\n\
    metadata:\n  name: app-canary\nspec:\n  targetRef:\n    apiVersion: apps/v1\n\
    \    kind: Deployment\n    name: app\n  progressDeadlineSeconds: 60\n  service:\n\
    \    port: 80\n    targetPort: 8080\n    gateways:\n    - public-gateway.istio-system.svc.cluster.local\n\
    \    hosts:\n    - app.example.com\n  analysis:\n    interval: 30s\n    threshold:\
    \ 5\n    maxWeight: 50\n    stepWeight: 10\n    metrics:\n    - name: request-success-rate\n\
    \      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n\
    \      thresholdRange:\n        max: 500\n      interval: 30s\n    webhooks:\n\
    \    - name: load-test\n      url: http://flagger-loadtester.test/\n      timeout:\
    \ 5s\n      metadata:\n        cmd: \"hey -z 1m -q 10 -c 2 http://app-canary.test:80/\"\
    \n```\n\n### 2.3 Pipeline Security\n\n#### Secret Management **[REQUIRED]**\n\
    ```yaml"
  .github/workflows/secrets-management.yml: "name: Secure Pipeline\n\nenv:\n  # Never\
    \ hardcode secrets\n  DATABASE_URL: ${{ secrets.DATABASE_URL }}\n  API_KEY: ${{\
    \ secrets.API_KEY }}\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n\
    \      - name: Configure HashiCorp Vault\n        uses: hashicorp/vault-action@v2\n\
    \        with:\n          url: https://vault.example.com\n          method: approle\n\
    \          roleId: ${{ secrets.VAULT_ROLE_ID }}\n          secretId: ${{ secrets.VAULT_SECRET_ID\
    \ }}\n          secrets: |\n            secret/data/app database_password | DB_PASSWORD\
    \ ;\n            secret/data/app api_key | API_KEY\n\n      - name: Use secrets\
    \ securely\n        run: |\n          # Secrets are available as environment variables\n\
    \          echo \"::add-mask::$DB_PASSWORD\"\n          echo \"::add-mask::$API_KEY\"\
    \n\n          # Use secrets in deployment\n          helm upgrade app ./charts/app\
    \ \\\n            --set database.password=\"$DB_PASSWORD\" \\\n            --set\
    \ api.key=\"$API_KEY\"\n```\n\n---"
  3. Container Orchestration: '### 3.1 Kubernetes Operations


    #### Resource Management **[REQUIRED]**

    ```yaml'
  kubernetes/base/deployment.yaml: "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n\
    \  name: app\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/version:\
    \ \"1.0.0\"\n    app.kubernetes.io/component: backend\nspec:\n  replicas: 3\n\
    \  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n\
    \      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app.kubernetes.io/name:\
    \ myapp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name:\
    \ myapp\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port:\
    \ \"8080\"\n        prometheus.io/path: \"/metrics\"\n    spec:\n      serviceAccountName:\
    \ app-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser:\
    \ 1000\n        fsGroup: 2000\n      containers:\n      - name: app\n        image:\
    \ myapp:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n       \
    \ - name: http\n          containerPort: 8080\n          protocol: TCP\n     \
    \   env:\n        - name: NODE_ENV\n          value: \"production\"\n        -\
    \ name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n      \
    \        name: app-secrets\n              key: database-url\n        resources:\n\
    \          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\
    \n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n\
    \        livenessProbe:\n          httpGet:\n            path: /health/live\n\
    \            port: http\n          initialDelaySeconds: 30\n          periodSeconds:\
    \ 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold:\
    \ 3\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n\
    \            port: http\n          initialDelaySeconds: 10\n          periodSeconds:\
    \ 5\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold:\
    \ 3\n        volumeMounts:\n        - name: config\n          mountPath: /etc/app\n\
    \          readOnly: true\n        - name: cache\n          mountPath: /tmp/cache\n\
    \      volumes:\n      - name: config\n        configMap:\n          name: app-config\n\
    \      - name: cache\n        emptyDir:\n          sizeLimit: 1Gi\n```\n\n####\
    \ Network Policies **[REQUIRED]**\n```yaml"
  kubernetes/network/policies.yaml: "apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\n\
    metadata:\n  name: app-network-policy\nspec:\n  podSelector:\n    matchLabels:\n\
    \      app.kubernetes.io/name: myapp\n  policyTypes:\n  - Ingress\n  - Egress\n\
    \  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n     \
    \     name: ingress-nginx\n    - podSelector:\n        matchLabels:\n        \
    \  app.kubernetes.io/name: frontend\n    ports:\n    - protocol: TCP\n      port:\
    \ 8080\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n \
    \         name: database\n    ports:\n    - protocol: TCP\n      port: 5432\n\
    \  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n\
    \          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n\
    ```\n\n### 3.2 Helm Packaging\n\n#### Chart Structure **[REQUIRED]**\n```yaml"
  charts/myapp/Chart.yaml: "apiVersion: v2\nname: myapp\ndescription: A Helm chart\
    \ for MyApp\ntype: application\nversion: 1.0.0\nappVersion: \"1.0.0\"\nhome: https://github.com/company/myapp\n\
    sources:\n  - https://github.com/company/myapp\nmaintainers:\n  - name: Platform\
    \ Team\n    email: platform@company.com\ndependencies:\n  - name: postgresql\n\
    \    version: \"~12.0.0\"\n    repository: https://charts.bitnami.com/bitnami\n\
    \    condition: postgresql.enabled\n  - name: redis\n    version: \"~17.0.0\"\n\
    \    repository: https://charts.bitnami.com/bitnami\n    condition: redis.enabled\n\
    ```\n\n#### Values Configuration **[REQUIRED]**\n```yaml"
  charts/myapp/values.yaml: ''
  Default values for myapp: "replicaCount: 3\n\nimage:\n  repository: mycompany/myapp\n\
    \  pullPolicy: IfNotPresent\n  tag: \"\"  # Overrides the image tag\n\nimagePullSecrets:\
    \ []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  create:\
    \ true\n  annotations: {}\n  name: \"\"\n\npodAnnotations: {}\n\npodSecurityContext:\n\
    \  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 2000\n\nsecurityContext:\n\
    \  capabilities:\n    drop:\n    - ALL\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation:\
    \ false\n\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: http\n  annotations:\
    \ {}\n\ningress:\n  enabled: false\n  className: \"nginx\"\n  annotations:\n \
    \   cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/rate-limit:\
    \ \"100\"\n  hosts:\n    - host: app.example.com\n      paths:\n        - path:\
    \ /\n          pathType: Prefix\n  tls:\n    - secretName: app-tls\n      hosts:\n\
    \        - app.example.com\n\nresources:\n  requests:\n    cpu: 100m\n    memory:\
    \ 256Mi\n  limits:\n    cpu: 500m\n    memory: 512Mi\n\nautoscaling:\n  enabled:\
    \ true\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage:\
    \ 70\n  targetMemoryUtilizationPercentage: 80\n\npostgresql:\n  enabled: true\n\
    \  auth:\n    database: myapp\n    existingSecret: postgresql-secret\n\nredis:\n\
    \  enabled: true\n  auth:\n    enabled: true\n    existingSecret: redis-secret\n\
    \nconfigMap:\n  NODE_ENV: production\n  LOG_LEVEL: info\n\nsecrets:\n  DATABASE_URL:\
    \ \"\"  # Set via --set or values file\n  REDIS_URL: \"\"\n  API_KEY: \"\"\n```\n\
    \n### 3.3 Service Mesh\n\n#### Istio Configuration **[RECOMMENDED]**\n```yaml"
  istio/virtual-service.yaml: "apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\n\
    metadata:\n  name: myapp\nspec:\n  hosts:\n  - myapp.example.com\n  gateways:\n\
    \  - myapp-gateway\n  http:\n  - match:\n    - headers:\n        x-version:\n\
    \          exact: v2\n    route:\n    - destination:\n        host: myapp\n  \
    \      subset: v2\n      weight: 100\n  - route:\n    - destination:\n       \
    \ host: myapp\n        subset: v1\n      weight: 90\n    - destination:\n    \
    \    host: myapp\n        subset: v2\n      weight: 10\n    timeout: 30s\n   \
    \ retries:\n      attempts: 3\n      perTryTimeout: 10s\n      retryOn: gateway-error,connect-failure,refused-stream\n\
    ---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n\
    \  name: myapp\nspec:\n  host: myapp\n  trafficPolicy:\n    connectionPool:\n\
    \      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests:\
    \ 10\n        http2MaxRequests: 100\n        maxRequestsPerConnection: 2\n   \
    \ loadBalancer:\n      simple: LEAST_REQUEST\n    outlierDetection:\n      consecutive5xxErrors:\
    \ 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent:\
    \ 50\n      minHealthPercent: 50\n  subsets:\n  - name: v1\n    labels:\n    \
    \  version: v1\n  - name: v2\n    labels:\n      version: v2\n```\n\n---"
  4. Platform Engineering: '### 4.1 Internal Developer Platform


    #### Platform Architecture **[REQUIRED]**

    ```yaml'
  platform/architecture.yaml: "platform:\n  name: \"Internal Developer Platform\"\n\
    \  version: \"2.0\"\n\n  components:\n    portal:\n      type: \"backstage\"\n\
    \      features:\n        - service_catalog\n        - api_documentation\n   \
    \     - tech_radar\n        - cost_insights\n\n    ci_cd:\n      type: \"tekton\"\
    \n      integrations:\n        - github\n        - gitlab\n        - bitbucket\n\
    \n    infrastructure:\n      provisioning: \"crossplane\"\n      providers:\n\
    \        - aws\n        - azure\n        - gcp\n\n    observability:\n      metrics:\
    \ \"prometheus\"\n      logs: \"loki\"\n      traces: \"tempo\"\n      visualization:\
    \ \"grafana\"\n\n    security:\n      scanning: \"trivy\"\n      policy: \"opa\"\
    \n      secrets: \"vault\"\n\n  self_service:\n    templates:\n      - microservice\n\
    \      - web_app\n      - data_pipeline\n      - ml_model\n\n    resources:\n\
    \      - database\n      - cache\n      - message_queue\n      - storage_bucket\n\
    ```\n\n#### Service Catalog **[REQUIRED]**\n```yaml"
  backstage/catalog-info.yaml: "apiVersion: backstage.io/v1alpha1\nkind: Component\n\
    metadata:\n  name: payment-service\n  description: Payment processing microservice\n\
    \  tags:\n    - java\n    - spring-boot\n    - payments\n  links:\n    - url:\
    \ https://dashboard.example.com/payment-service\n      title: Monitoring Dashboard\n\
    \    - url: https://wiki.example.com/payment-service\n      title: Documentation\n\
    \  annotations:\n    github.com/project-slug: company/payment-service\n    prometheus.io/rule:\
    \ 'payment_error_rate > 0.01'\n    pagerduty.com/integration-key: ${PAGERDUTY_KEY}\n\
    spec:\n  type: service\n  lifecycle: production\n  owner: team-payments\n  system:\
    \ payment-platform\n  dependsOn:\n    - component:default/user-service\n    -\
    \ resource:default/payment-database\n  providesApis:\n    - payment-api\n---\n\
    apiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: payment-api\n\
    \  description: Payment Service API\nspec:\n  type: openapi\n  lifecycle: production\n\
    \  owner: team-payments\n  definition:\n    $text: ./openapi/payment-api.yaml\n\
    ```\n\n#### Developer Experience **[REQUIRED]**\n```typescript\n// platform/cli/src/commands/create-service.ts\n\
    import { Command } from 'commander';\nimport { generateFromTemplate } from '../templates';\n\
    import { provisionInfrastructure } from '../infrastructure';\nimport { setupPipeline\
    \ } from '../cicd';\n\nexport const createServiceCommand = new Command('create-service')\n\
    \  .description('Create a new microservice with all platform integrations')\n\
    \  .option('-n, --name <name>', 'Service name')\n  .option('-t, --template <template>',\
    \ 'Service template', 'microservice')\n  .option('-l, --language <language>',\
    \ 'Programming language', 'typescript')\n  .option('--with-database', 'Include\
    \ database', false)\n  .option('--with-cache', 'Include cache', false)\n  .action(async\
    \ (options) => {\n    console.log('\U0001F680 Creating new service...');\n\n \
    \   // Generate code from template\n    const servicePath = await generateFromTemplate({\n\
    \      template: options.template,\n      language: options.language,\n      name:\
    \ options.name,\n      features: {\n        database: options.withDatabase,\n\
    \        cache: options.withCache,\n      },\n    });\n\n    // Provision infrastructure\n\
    \    const infrastructure = await provisionInfrastructure({\n      service: options.name,\n\
    \      resources: {\n        kubernetes: {\n          namespace: options.name,\n\
    \          serviceAccount: true,\n        },\n        database: options.withDatabase\
    \ ? {\n          type: 'postgresql',\n          size: 'small',\n        } : undefined,\n\
    \        cache: options.withCache ? {\n          type: 'redis',\n          size:\
    \ 'small',\n        } : undefined,\n      },\n    });\n\n    // Setup CI/CD pipeline\n\
    \    const pipeline = await setupPipeline({\n      service: options.name,\n  \
    \    repository: `company/${options.name}`,\n      stages: ['build', 'test', 'scan',\
    \ 'deploy'],\n      environments: ['dev', 'staging', 'prod'],\n    });\n\n   \
    \ console.log('\u2705 Service created successfully!');\n    console.log(`\U0001F4C1\
    \ Code: ${servicePath}`);\n    console.log(`\U0001F3D7\uFE0F  Infrastructure:\
    \ ${infrastructure.dashboardUrl}`);\n    console.log(`\U0001F504 Pipeline: ${pipeline.url}`);\n\
    \  });\n```\n\n### 4.2 Self-Service Infrastructure\n\n#### Resource Templates\
    \ **[REQUIRED]**\n```yaml"
  platform/templates/database.yaml: "apiVersion: platform.io/v1\nkind: ResourceTemplate\n\
    metadata:\n  name: postgresql-database\n  description: PostgreSQL database instance\n\
    spec:\n  parameters:\n    - name: size\n      type: string\n      description:\
    \ Database size (small, medium, large)\n      default: small\n      enum: [small,\
    \ medium, large]\n    - name: version\n      type: string\n      description:\
    \ PostgreSQL version\n      default: \"15\"\n      enum: [\"13\", \"14\", \"15\"\
    ]\n    - name: highAvailability\n      type: boolean\n      description: Enable\
    \ high availability\n      default: false\n\n  resources:\n    - apiVersion: postgresql.cnpg.io/v1\n\
    \      kind: Cluster\n      metadata:\n        name: \"{{ .Name }}-postgres\"\n\
    \        namespace: \"{{ .Namespace }}\"\n      spec:\n        instances: \"{{\
    \ if .Values.highAvailability }}3{{ else }}1{{ end }}\"\n        postgresql:\n\
    \          parameters:\n            max_connections: \"200\"\n            shared_buffers:\
    \ \"256MB\"\n        bootstrap:\n          initdb:\n            database: \"{{\
    \ .Name }}\"\n            owner: \"{{ .Name }}\"\n            secret:\n      \
    \        name: \"{{ .Name }}-postgres-secret\"\n        storage:\n          size:\
    \ |\n            {{- if eq .Values.size \"small\" }}10Gi\n            {{- else\
    \ if eq .Values.size \"medium\" }}50Gi\n            {{- else if eq .Values.size\
    \ \"large\" }}200Gi\n            {{- end }}\n        monitoring:\n          enabled:\
    \ true\n        backup:\n          enabled: true\n          retentionPolicy: \"\
    30d\"\n```\n\n#### Platform API **[REQUIRED]**\n```go\n// platform/api/handlers/resource.go\n\
    package handlers\n\nimport (\n    \"net/http\"\n    \"github.com/gin-gonic/gin\"\
    \n    \"platform/internal/resources\"\n    \"platform/internal/auth\"\n)\n\ntype\
    \ CreateResourceRequest struct {\n    Type       string                 `json:\"\
    type\" binding:\"required\"`\n    Name       string                 `json:\"name\"\
    \ binding:\"required\"`\n    Team       string                 `json:\"team\"\
    \ binding:\"required\"`\n    Parameters map[string]interface{} `json:\"parameters\"\
    `\n}\n\nfunc CreateResource(c *gin.Context) {\n    var req CreateResourceRequest\n\
    \    if err := c.ShouldBindJSON(&req); err != nil {\n        c.JSON(http.StatusBadRequest,\
    \ gin.H{\"error\": err.Error()})\n        return\n    }\n\n    // Validate team\
    \ ownership\n    user := auth.GetUser(c)\n    if !user.IsInTeam(req.Team) {\n\
    \        c.JSON(http.StatusForbidden, gin.H{\"error\": \"not authorized for team\"\
    })\n        return\n    }\n\n    // Create resource\n    resource, err := resources.Create(c.Request.Context(),\
    \ resources.CreateOptions{\n        Type:       req.Type,\n        Name:     \
    \  req.Name,\n        Team:       req.Team,\n        Parameters: req.Parameters,\n\
    \        CreatedBy:  user.Email,\n    })\n    if err != nil {\n        c.JSON(http.StatusInternalServerError,\
    \ gin.H{\"error\": err.Error()})\n        return\n    }\n\n    // Audit log\n\
    \    audit.Log(c.Request.Context(), audit.Entry{\n        Action:   \"resource.create\"\
    ,\n        Resource: resource.ID,\n        User:     user.Email,\n        Team:\
    \     req.Team,\n    })\n\n    c.JSON(http.StatusCreated, gin.H{\n        \"id\"\
    :          resource.ID,\n        \"status\":      resource.Status,\n        \"\
    dashboardUrl\": resource.DashboardURL,\n        \"credentials\": resource.Credentials,\n\
    \    })\n}\n```\n\n---"
  5. Site Reliability Engineering: '### 5.1 Service Level Objectives


    #### SLO Definition **[REQUIRED]**

    ```yaml'
  sre/slos/payment-service.yaml: "apiVersion: sloth.slok.dev/v1\nkind: PrometheusServiceLevel\n\
    metadata:\n  name: payment-service\n  labels:\n    team: payments\n    tier: critical\n\
    spec:\n  service: payment-service\n  labels:\n    team: payments\n  slos:\n  \
    \  - name: availability\n      objective: 99.9\n      description: \"99.9% of\
    \ requests should be successful\"\n      sli:\n        events:\n          error_query:\
    \ |\n            sum(rate(http_requests_total{\n              service=\"payment-service\"\
    ,\n              status=~\"5..\"\n            }[5m]))\n          total_query:\
    \ |\n            sum(rate(http_requests_total{\n              service=\"payment-service\"\
    \n            }[5m]))\n      alerting:\n        name: PaymentServiceAvailability\n\
    \        page_alert:\n          labels:\n            severity: critical\n    \
    \        team: payments\n        ticket_alert:\n          labels:\n          \
    \  severity: warning\n            team: payments\n\n    - name: latency\n    \
    \  objective: 99\n      description: \"99% of requests should complete within\
    \ 500ms\"\n      sli:\n        events:\n          error_query: |\n           \
    \ sum(rate(http_request_duration_seconds_bucket{\n              service=\"payment-service\"\
    ,\n              le=\"0.5\"\n            }[5m]))\n          total_query: |\n \
    \           sum(rate(http_request_duration_seconds_count{\n              service=\"\
    payment-service\"\n            }[5m]))\n      alerting:\n        name: PaymentServiceLatency\n\
    \        page_alert:\n          labels:\n            severity: critical\n```\n\
    \n#### Error Budget Policy **[REQUIRED]**\n```yaml"
  sre/policies/error-budget.yaml: "error_budget_policy:\n  payment_service:\n    slo_target:\
    \ 99.9\n    measurement_window: 30d\n\n    thresholds:\n      - remaining_budget:\
    \ 75%\n        actions:\n          - type: notification\n            target: team-slack\n\
    \            message: \"Error budget at 75% - review recent changes\"\n\n    \
    \  - remaining_budget: 50%\n        actions:\n          - type: review\n     \
    \       description: \"Mandatory review of recent deployments\"\n          - type:\
    \ slow_rollout\n            description: \"Reduce deployment velocity to 1/day\"\
    \n\n      - remaining_budget: 25%\n        actions:\n          - type: freeze\n\
    \            description: \"Feature freeze - only critical fixes\"\n         \
    \ - type: incident_review\n            description: \"Conduct thorough incident\
    \ analysis\"\n\n      - remaining_budget: 0%\n        actions:\n          - type:\
    \ halt_deployments\n            description: \"Stop all non-emergency deployments\"\
    \n          - type: dedicated_response\n            description: \"Assign dedicated\
    \ team to reliability\"\n\n    exemptions:\n      - type: security_patch\n   \
    \     approval: security_team\n      - type: data_corruption_fix\n        approval:\
    \ engineering_director\n```\n\n### 5.2 Incident Management\n\n#### Incident Response\
    \ **[REQUIRED]**\n```yaml"
  sre/runbooks/incident-response.yaml: "incident_response:\n  roles:\n    incident_commander:\n\
    \      responsibilities:\n        - Overall incident coordination\n        - External\
    \ communication\n        - Decision making\n      backup: platform_lead\n\n  \
    \  technical_lead:\n      responsibilities:\n        - Technical investigation\n\
    \        - Solution implementation\n        - Root cause analysis\n      backup:\
    \ senior_engineer\n\n    communications_lead:\n      responsibilities:\n     \
    \   - Status page updates\n        - Customer communication\n        - Internal\
    \ updates\n      backup: product_manager\n\n  severities:\n    sev1:\n      definition:\
    \ \"Complete service outage or data loss\"\n      response_time: 5_minutes\n \
    \     escalation:\n        - oncall_engineer\n        - team_lead\n        - engineering_director\n\
    \        - cto\n      communication:\n        - status_page: immediate\n     \
    \   - customer_email: 15_minutes\n        - executive_brief: 30_minutes\n\n  \
    \  sev2:\n      definition: \"Major feature unavailable or significant degradation\"\
    \n      response_time: 15_minutes\n      escalation:\n        - oncall_engineer\n\
    \        - team_lead\n      communication:\n        - status_page: 15_minutes\n\
    \        - customer_email: 1_hour\n\n    sev3:\n      definition: \"Minor feature\
    \ unavailable or minor degradation\"\n      response_time: 1_hour\n      escalation:\n\
    \        - oncall_engineer\n      communication:\n        - status_page: 1_hour\n\
    \n  procedures:\n    initial_response:\n      - Acknowledge alert within SLA\n\
    \      - Assess severity and impact\n      - Create incident channel\n      -\
    \ Assign roles\n      - Begin investigation\n\n    investigation:\n      - Review\
    \ monitoring dashboards\n      - Check recent deployments\n      - Analyze logs\
    \ and traces\n      - Test hypotheses\n      - Document findings\n\n    mitigation:\n\
    \      - Implement immediate fixes\n      - Consider rollback if needed\n    \
    \  - Monitor impact of changes\n      - Update stakeholders\n\n    resolution:\n\
    \      - Verify issue is resolved\n      - Monitor for recurrence\n      - Update\
    \ status page\n      - Schedule post-mortem\n```\n\n#### Runbook Automation **[REQUIRED]**\n\
    ```python"
  sre/automation/runbooks/database_recovery.py: "#!/usr/bin/env python3\n\"\"\"\n\
    Automated database recovery runbook\n\"\"\"\nimport logging\nimport time\nfrom\
    \ typing import Dict, List\nimport boto3\nimport psycopg2\nfrom datadog import\
    \ api\nfrom slack_sdk import WebClient\n\nlogger = logging.getLogger(__name__)\n\
    \nclass DatabaseRecoveryRunbook:\n    def __init__(self):\n        self.rds_client\
    \ = boto3.client('rds')\n        self.cloudwatch = boto3.client('cloudwatch')\n\
    \        self.slack = WebClient(token=os.environ['SLACK_TOKEN'])\n        self.incident_channel\
    \ = None\n\n    def execute(self, alert_data: Dict) -> None:\n        \"\"\"Execute\
    \ database recovery runbook\"\"\"\n        try:\n            # 1. Create incident\
    \ channel\n            self.incident_channel = self._create_incident_channel(alert_data)\n\
    \n            # 2. Assess database state\n            db_state = self._assess_database_state(alert_data['db_identifier'])\n\
    \            self._post_to_slack(f\"Database state: {db_state}\")\n\n        \
    \    # 3. Determine recovery action\n            if db_state['status'] == 'failed':\n\
    \                self._initiate_failover(alert_data['db_identifier'])\n      \
    \      elif db_state['connections'] > db_state['max_connections'] * 0.9:\n   \
    \             self._terminate_idle_connections(alert_data['db_identifier'])\n\
    \            elif db_state['cpu_utilization'] > 90:\n                self._scale_up_instance(alert_data['db_identifier'])\n\
    \n            # 4. Monitor recovery\n            self._monitor_recovery(alert_data['db_identifier'])\n\
    \n            # 5. Validate recovery\n            if self._validate_recovery(alert_data['db_identifier']):\n\
    \                self._post_to_slack(\"\u2705 Database recovered successfully\"\
    )\n                self._close_incident()\n            else:\n               \
    \ self._escalate_incident()\n\n        except Exception as e:\n            logger.error(f\"\
    Runbook execution failed: {e}\")\n            self._escalate_incident()\n\n  \
    \  def _assess_database_state(self, db_identifier: str) -> Dict:\n        \"\"\
    \"Assess current database state\"\"\"\n        # Get RDS instance status\n   \
    \     response = self.rds_client.describe_db_instances(\n            DBInstanceIdentifier=db_identifier\n\
    \        )\n        instance = response['DBInstances'][0]\n\n        # Get performance\
    \ metrics\n        metrics = self._get_performance_metrics(db_identifier)\n\n\
    \        # Check replica lag if applicable\n        replica_lag = self._check_replica_lag(db_identifier)\n\
    \n        return {\n            'status': instance['DBInstanceStatus'],\n    \
    \        'endpoint': instance.get('Endpoint', {}).get('Address'),\n          \
    \  'connections': metrics['DatabaseConnections'],\n            'max_connections':\
    \ self._get_max_connections(instance),\n            'cpu_utilization': metrics['CPUUtilization'],\n\
    \            'replica_lag': replica_lag,\n            'storage_free': metrics['FreeStorageSpace'],\n\
    \        }\n\n    def _initiate_failover(self, db_identifier: str) -> None:\n\
    \        \"\"\"Initiate database failover\"\"\"\n        self._post_to_slack(\"\
    \U0001F504 Initiating database failover...\")\n\n        try:\n            # For\
    \ Multi-AZ deployments\n            self.rds_client.reboot_db_instance(\n    \
    \            DBInstanceIdentifier=db_identifier,\n                ForceFailover=True\n\
    \            )\n\n            # Wait for failover to complete\n            waiter\
    \ = self.rds_client.get_waiter('db_instance_available')\n            waiter.wait(\n\
    \                DBInstanceIdentifier=db_identifier,\n                WaiterConfig={\n\
    \                    'Delay': 30,\n                    'MaxAttempts': 40\n   \
    \             }\n            )\n\n            self._post_to_slack(\"\u2705 Failover\
    \ completed successfully\")\n\n        except Exception as e:\n            logger.error(f\"\
    Failover failed: {e}\")\n            raise\n```\n\n### 5.3 Chaos Engineering\n\
    \n#### Chaos Experiments **[RECOMMENDED]**\n```yaml"
  chaos/experiments/payment-service.yaml: "apiVersion: chaos-mesh.org/v1alpha1\nkind:\
    \ Schedule\nmetadata:\n  name: payment-service-chaos\n  namespace: chaos-testing\n\
    spec:\n  schedule: \"0 10 * * 1-5\"  # Weekdays at 10 AM\n  concurrencyPolicy:\
    \ Forbid\n  type: PodChaos\n  podChaos:\n    action: pod-kill\n    mode: one\n\
    \    duration: \"60s\"\n    selector:\n      namespaces:\n        - production\n\
    \      labelSelectors:\n        \"app.kubernetes.io/name\": \"payment-service\"\
    \n    scheduler:\n      cron: \"@hourly\"\n---\napiVersion: chaos-mesh.org/v1alpha1\n\
    kind: NetworkChaos\nmetadata:\n  name: payment-network-delay\nspec:\n  action:\
    \ delay\n  mode: all\n  selector:\n    namespaces:\n      - production\n    labelSelectors:\n\
    \      \"app.kubernetes.io/name\": \"payment-service\"\n  delay:\n    latency:\
    \ \"100ms\"\n    correlation: \"25\"\n    jitter: \"10ms\"\n  duration: \"5m\"\
    \n  scheduler:\n    cron: \"0 */4 * * *\"\n---\napiVersion: chaos-mesh.org/v1alpha1\n\
    kind: StressChaos\nmetadata:\n  name: payment-cpu-stress\nspec:\n  mode: one\n\
    \  selector:\n    namespaces:\n      - production\n    labelSelectors:\n     \
    \ \"app.kubernetes.io/name\": \"payment-service\"\n  stressors:\n    cpu:\n  \
    \    workers: 2\n      load: 80\n  duration: \"3m\"\n```\n\n#### Gameday Automation\
    \ **[RECOMMENDED]**\n```python"
  chaos/gameday/scenarios.py: 'import asyncio

    from typing import List, Dict

    import structlog

    from chaos_toolkit import run_experiment

    from prometheus_client import Gauge


    logger = structlog.get_logger()'
  Metrics: "gameday_score = Gauge('gameday_score', 'Gameday scenario score', ['scenario'])\n\
    gameday_mttr = Gauge('gameday_mttr', 'Mean time to recovery', ['scenario'])\n\n\
    class GamedayScenario:\n    def __init__(self, name: str, description: str):\n\
    \        self.name = name\n        self.description = description\n        self.start_time\
    \ = None\n        self.recovery_time = None\n\n    async def run(self) -> Dict:\n\
    \        \"\"\"Run gameday scenario\"\"\"\n        logger.info(\"Starting gameday\
    \ scenario\", scenario=self.name)\n        self.start_time = asyncio.get_event_loop().time()\n\
    \n        try:\n            # Run chaos experiment\n            result = await\
    \ self._execute_chaos()\n\n            # Monitor system behavior\n           \
    \ impact = await self._monitor_impact()\n\n            # Verify auto-recovery\n\
    \            recovery = await self._verify_recovery()\n\n            # Calculate\
    \ metrics\n            self.recovery_time = asyncio.get_event_loop().time()\n\
    \            mttr = self.recovery_time - self.start_time\n\n            # Score\
    \ the scenario\n            score = self._calculate_score(impact, recovery, mttr)\n\
    \n            # Update metrics\n            gameday_score.labels(scenario=self.name).set(score)\n\
    \            gameday_mttr.labels(scenario=self.name).set(mttr)\n\n           \
    \ return {\n                'scenario': self.name,\n                'success':\
    \ True,\n                'score': score,\n                'mttr': mttr,\n    \
    \            'impact': impact,\n                'recovery': recovery,\n      \
    \      }\n\n        except Exception as e:\n            logger.error(\"Gameday\
    \ scenario failed\", scenario=self.name, error=str(e))\n            return {\n\
    \                'scenario': self.name,\n                'success': False,\n \
    \               'error': str(e),\n            }\n\n    async def _execute_chaos(self)\
    \ -> Dict:\n        \"\"\"Execute chaos experiment\"\"\"\n        experiment =\
    \ {\n            \"version\": \"1.0.0\",\n            \"title\": f\"Gameday: {self.name}\"\
    ,\n            \"description\": self.description,\n            \"steady-state-hypothesis\"\
    : {\n                \"title\": \"System is healthy\",\n                \"probes\"\
    : [\n                    {\n                        \"type\": \"probe\",\n   \
    \                     \"name\": \"service-available\",\n                     \
    \   \"provider\": {\n                            \"type\": \"http\",\n       \
    \                     \"url\": \"http://payment-service/health\",\n          \
    \                  \"timeout\": 5,\n                        },\n             \
    \       },\n                ],\n            },\n            \"method\": [\n  \
    \              {\n                    \"type\": \"action\",\n                \
    \    \"name\": \"inject-failure\",\n                    \"provider\": {\n    \
    \                    \"type\": \"python\",\n                        \"module\"\
    : \"chaosaws.ec2.actions\",\n                        \"func\": \"terminate_instances\"\
    ,\n                        \"arguments\": {\n                            \"instance_ids\"\
    : [\"i-1234567890abcdef0\"],\n                        },\n                   \
    \ },\n                },\n            ],\n            \"rollbacks\": [],\n   \
    \     }\n\n        return await asyncio.to_thread(run_experiment, experiment)\n\
    ```\n\n---"
  6. GitOps and Deployment: '### 6.1 GitOps Workflows


    #### ArgoCD Configuration **[REQUIRED]**

    ```yaml'
  argocd/applications/production.yaml: "apiVersion: argoproj.io/v1alpha1\nkind: Application\n\
    metadata:\n  name: payment-service-prod\n  namespace: argocd\n  finalizers:\n\
    \    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: production\n\
    \  source:\n    repoURL: https://github.com/company/payment-service\n    targetRevision:\
    \ main\n    path: kubernetes/overlays/production\n  destination:\n    server:\
    \ https://kubernetes.default.svc\n    namespace: payment-service\n  syncPolicy:\n\
    \    automated:\n      prune: true\n      selfHeal: true\n      allowEmpty: false\n\
    \    syncOptions:\n    - CreateNamespace=true\n    - PrunePropagationPolicy=foreground\n\
    \    - PruneLast=true\n    retry:\n      limit: 5\n      backoff:\n        duration:\
    \ 5s\n        factor: 2\n        maxDuration: 3m\n  revisionHistoryLimit: 10\n\
    \  ignoreDifferences:\n  - group: apps\n    kind: Deployment\n    jsonPointers:\n\
    \    - /spec/replicas\n  - group: autoscaling\n    kind: HorizontalPodAutoscaler\n\
    \    jsonPointers:\n    - /spec/minReplicas\n    - /spec/maxReplicas\n```\n\n\
    #### Flux Configuration **[ALTERNATIVE]**\n```yaml"
  flux/clusters/production/payment-service.yaml: "apiVersion: source.toolkit.fluxcd.io/v1beta2\n\
    kind: GitRepository\nmetadata:\n  name: payment-service\n  namespace: flux-system\n\
    spec:\n  interval: 1m\n  ref:\n    branch: main\n  url: https://github.com/company/payment-service\n\
    \  secretRef:\n    name: github-token\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\n\
    kind: Kustomization\nmetadata:\n  name: payment-service\n  namespace: flux-system\n\
    spec:\n  interval: 10m\n  path: \"./kubernetes/overlays/production\"\n  prune:\
    \ true\n  sourceRef:\n    kind: GitRepository\n    name: payment-service\n  validation:\
    \ client\n  timeout: 5m\n  retryInterval: 2m\n  targetNamespace: payment-service\n\
    \  postBuild:\n    substitute:\n      cluster_env: production\n      region: us-east-1\n\
    \    substituteFrom:\n    - kind: ConfigMap\n      name: cluster-config\n    -\
    \ kind: Secret\n      name: cluster-secrets\n  healthChecks:\n  - apiVersion:\
    \ apps/v1\n    kind: Deployment\n    name: payment-service\n    namespace: payment-service\n\
    \  - apiVersion: v1\n    kind: Service\n    name: payment-service\n    namespace:\
    \ payment-service\n```\n\n### 6.2 Progressive Delivery\n\n#### Flagger Configuration\
    \ **[RECOMMENDED]**\n```yaml"
  flagger/canary/payment-service.yaml: "apiVersion: flagger.app/v1beta1\nkind: Canary\n\
    metadata:\n  name: payment-service\n  namespace: production\nspec:\n  provider:\
    \ istio\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name:\
    \ payment-service\n  progressDeadlineSeconds: 600\n  service:\n    port: 80\n\
    \    targetPort: 8080\n    gateways:\n    - public-gateway.istio-system.svc.cluster.local\n\
    \    hosts:\n    - payment.example.com\n    trafficPolicy:\n      tls:\n     \
    \   mode: SIMPLE\n    match:\n    - headers:\n        x-canary:\n          exact:\
    \ \"true\"\n  analysis:\n    interval: 1m\n    threshold: 10\n    maxWeight: 50\n\
    \    stepWeight: 5\n    stepWeightPromotion: 10\n    metrics:\n    - name: request-success-rate\n\
    \      templateRef:\n        name: success-rate\n        namespace: flagger-system\n\
    \      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n\
    \      templateRef:\n        name: latency\n        namespace: flagger-system\n\
    \      thresholdRange:\n        max: 500\n      interval: 1m\n    - name: error-rate\n\
    \      templateRef:\n        name: error-rate\n        namespace: flagger-system\n\
    \      thresholdRange:\n        max: 1\n      interval: 1m\n    webhooks:\n  \
    \  - name: smoke-test\n      type: pre-rollout\n      url: http://flagger-loadtester.test/\n\
    \      timeout: 15s\n      metadata:\n        type: smoke\n        cmd: \"curl\
    \ -s http://payment-service-canary/health\"\n    - name: load-test\n      type:\
    \ rollout\n      url: http://flagger-loadtester.test/\n      timeout: 5s\n   \
    \   metadata:\n        type: cmd\n        cmd: \"hey -z 1m -q 100 -c 10 http://payment-service-canary/\"\
    \n    - name: rollback-notification\n      type: rollback\n      url: http://webhook.example.com/\n\
    \      metadata:\n        environment: production\n        service: payment-service\n\
    \  alerts:\n  - name: \"payment-service canary\"\n    severity: info\n    providerRef:\n\
    \      name: slack\n      namespace: flagger-system\n```\n\n---"
  7. Configuration Management: '### 7.1 Configuration Standards


    #### External Configuration **[REQUIRED]**

    ```yaml'
  kubernetes/configmaps/app-config.yaml: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n\
    \  name: payment-service-config\n  namespace: production\n  labels:\n    app.kubernetes.io/name:\
    \ payment-service\n    app.kubernetes.io/component: configuration\ndata:\n  application.yaml:\
    \ |\n    server:\n      port: 8080\n      shutdown: graceful\n\n    spring:\n\
    \      application:\n        name: payment-service\n\n      datasource:\n    \
    \    hikari:\n          maximum-pool-size: 20\n          minimum-idle: 5\n   \
    \       connection-timeout: 30000\n          idle-timeout: 600000\n          max-lifetime:\
    \ 1800000\n\n    management:\n      endpoints:\n        web:\n          exposure:\n\
    \            include: health,info,metrics,prometheus\n      metrics:\n       \
    \ export:\n          prometheus:\n            enabled: true\n\n    resilience4j:\n\
    \      circuitbreaker:\n        instances:\n          payment-gateway:\n     \
    \       sliding-window-size: 100\n            permitted-number-of-calls-in-half-open-state:\
    \ 10\n            wait-duration-in-open-state: 60000\n            failure-rate-threshold:\
    \ 50\n            slow-call-rate-threshold: 50\n            slow-call-duration-threshold:\
    \ 2000\n\n      retry:\n        instances:\n          payment-gateway:\n     \
    \       max-attempts: 3\n            wait-duration: 1000\n            retry-exceptions:\n\
    \              - java.net.SocketTimeoutException\n              - java.io.IOException\n\
    ```\n\n#### Secret Management **[REQUIRED]**\n```yaml"
  kubernetes/secrets/sealed-secrets.yaml: "apiVersion: bitnami.com/v1alpha1\nkind:\
    \ SealedSecret\nmetadata:\n  name: payment-service-secrets\n  namespace: production\n\
    spec:\n  encryptedData:\n    database-url: AgBvA7sF2H... # Encrypted value\n \
    \   api-key: AgCpQ9kL3M... # Encrypted value\n    jwt-secret: AgDrS4mN5P... #\
    \ Encrypted value\n  template:\n    metadata:\n      name: payment-service-secrets\n\
    \      namespace: production\n    type: Opaque\n```\n\n#### Dynamic Configuration\
    \ **[RECOMMENDED]**\n```go\n// config/dynamic/client.go\npackage dynamic\n\nimport\
    \ (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n\n    \"github.com/hashicorp/consul/api\"\
    \n    \"go.uber.org/zap\"\n)\n\ntype ConfigClient struct {\n    consul      *api.Client\n\
    \    logger      *zap.Logger\n    cache       sync.Map\n    subscribers map[string][]chan\
    \ string\n    mu          sync.RWMutex\n}\n\nfunc NewConfigClient(consulAddr string,\
    \ logger *zap.Logger) (*ConfigClient, error) {\n    config := api.DefaultConfig()\n\
    \    config.Address = consulAddr\n\n    client, err := api.NewClient(config)\n\
    \    if err != nil {\n        return nil, fmt.Errorf(\"failed to create consul\
    \ client: %w\", err)\n    }\n\n    cc := &ConfigClient{\n        consul:     \
    \ client,\n        logger:      logger,\n        subscribers: make(map[string][]chan\
    \ string),\n    }\n\n    // Start watching for changes\n    go cc.watchChanges(context.Background())\n\
    \n    return cc, nil\n}\n\nfunc (c *ConfigClient) Get(key string) (string, error)\
    \ {\n    // Check cache first\n    if value, ok := c.cache.Load(key); ok {\n \
    \       return value.(string), nil\n    }\n\n    // Fetch from Consul\n    kv\
    \ := c.consul.KV()\n    pair, _, err := kv.Get(key, nil)\n    if err != nil {\n\
    \        return \"\", fmt.Errorf(\"failed to get key %s: %w\", key, err)\n   \
    \ }\n\n    if pair == nil {\n        return \"\", fmt.Errorf(\"key %s not found\"\
    , key)\n    }\n\n    value := string(pair.Value)\n    c.cache.Store(key, value)\n\
    \n    return value, nil\n}\n\nfunc (c *ConfigClient) Watch(key string) <-chan\
    \ string {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n\n    ch := make(chan string,\
    \ 1)\n    c.subscribers[key] = append(c.subscribers[key], ch)\n\n    // Send current\
    \ value\n    if value, err := c.Get(key); err == nil {\n        ch <- value\n\
    \    }\n\n    return ch\n}\n\nfunc (c *ConfigClient) watchChanges(ctx context.Context)\
    \ {\n    kv := c.consul.KV()\n\n    for {\n        select {\n        case <-ctx.Done():\n\
    \            return\n        default:\n            keys, _, err := kv.List(\"\"\
    , nil)\n            if err != nil {\n                c.logger.Error(\"Failed to\
    \ list keys\", zap.Error(err))\n                time.Sleep(5 * time.Second)\n\
    \                continue\n            }\n\n            for _, key := range keys\
    \ {\n                oldValue, _ := c.cache.Load(key.Key)\n                newValue\
    \ := string(key.Value)\n\n                if oldValue != newValue {\n        \
    \            c.cache.Store(key.Key, newValue)\n                    c.notifySubscribers(key.Key,\
    \ newValue)\n                }\n            }\n\n            time.Sleep(10 * time.Second)\n\
    \        }\n    }\n}\n```\n\n### 7.2 Feature Flags\n\n#### Feature Flag System\
    \ **[RECOMMENDED]**\n```yaml"
  featureflags/flags.yaml: "flags:\n  - key: new-payment-provider\n    description:\
    \ Enable new payment provider integration\n    enabled: false\n    rules:\n  \
    \    - type: percentage\n        enabled: true\n        percentage: 10\n     \
    \   conditions:\n          - attribute: region\n            operator: in\n   \
    \         values: [us-east-1, us-west-2]\n\n      - type: user-list\n        enabled:\
    \ true\n        users:\n          - user123\n          - user456\n\n      - type:\
    \ group\n        enabled: true\n        groups:\n          - beta-testers\n  \
    \        - internal-users\n\n  - key: enhanced-fraud-detection\n    description:\
    \ Use ML-based fraud detection\n    enabled: true\n    rules:\n      - type: gradual-rollout\n\
    \        enabled: true\n        stages:\n          - percentage: 5\n         \
    \   duration: 1h\n          - percentage: 25\n            duration: 24h\n    \
    \      - percentage: 50\n            duration: 72h\n          - percentage: 100\n\
    ```\n\n---"
  8. Release Management: '### 8.1 Versioning Strategy


    #### Semantic Versioning **[REQUIRED]**

    ```yaml'
  .github/workflows/release.yml: "name: Release\n\non:\n  push:\n    branches: [main]\n\
    \njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\
    \        with:\n          fetch-depth: 0\n\n      - name: Calculate Version\n\
    \        id: version\n        uses: mathieudutour/github-tag-action@v6.1\n   \
    \     with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          default_bump:\
    \ patch\n          release_branches: main\n          pre_release_branches: develop\n\
    \          custom_release_rules: |\n            breaking:major:Breaking Changes\n\
    \            feat:minor:Features\n            fix:patch:Bug Fixes\n          \
    \  perf:patch:Performance\n\n      - name: Generate Changelog\n        id: changelog\n\
    \        run: |\n          npm install -g conventional-changelog-cli\n       \
    \   conventional-changelog -p angular -i CHANGELOG.md -s -r 0\n\n      - name:\
    \ Create Release\n        uses: ncipollo/release-action@v1\n        with:\n  \
    \        tag: ${{ steps.version.outputs.new_tag }}\n          name: Release ${{\
    \ steps.version.outputs.new_tag }}\n          body: ${{ steps.version.outputs.changelog\
    \ }}\n          artifacts: |\n            dist/*\n            checksums.txt\n\
    ```\n\n#### Release Notes Automation **[REQUIRED]**\n```typescript\n// scripts/generate-release-notes.ts\n\
    import { Octokit } from '@octokit/rest';\nimport { getChangelogEntry } from 'conventional-changelog';\n\
    \ninterface ReleaseNotes {\n  version: string;\n  date: string;\n  breaking: string[];\n\
    \  features: string[];\n  fixes: string[];\n  performance: string[];\n  security:\
    \ string[];\n  dependencies: string[];\n}\n\nasync function generateReleaseNotes(\n\
    \  owner: string,\n  repo: string,\n  fromTag: string,\n  toTag: string\n): Promise<ReleaseNotes>\
    \ {\n  const octokit = new Octokit({\n    auth: process.env.GITHUB_TOKEN,\n  });\n\
    \n  // Get commits between tags\n  const commits = await octokit.repos.compareCommits({\n\
    \    owner,\n    repo,\n    base: fromTag,\n    head: toTag,\n  });\n\n  // Parse\
    \ commit messages\n  const notes: ReleaseNotes = {\n    version: toTag,\n    date:\
    \ new Date().toISOString(),\n    breaking: [],\n    features: [],\n    fixes:\
    \ [],\n    performance: [],\n    security: [],\n    dependencies: [],\n  };\n\n\
    \  for (const commit of commits.data.commits) {\n    const message = commit.commit.message;\n\
    \n    if (message.includes('BREAKING CHANGE:')) {\n      notes.breaking.push(extractDescription(message));\n\
    \    } else if (message.startsWith('feat:')) {\n      notes.features.push(extractDescription(message));\n\
    \    } else if (message.startsWith('fix:')) {\n      notes.fixes.push(extractDescription(message));\n\
    \    } else if (message.startsWith('perf:')) {\n      notes.performance.push(extractDescription(message));\n\
    \    } else if (message.startsWith('security:')) {\n      notes.security.push(extractDescription(message));\n\
    \    } else if (message.startsWith('deps:')) {\n      notes.dependencies.push(extractDescription(message));\n\
    \    }\n  }\n\n  return notes;\n}\n\nfunction formatReleaseNotes(notes: ReleaseNotes):\
    \ string {\n  let output = `# Release ${notes.version}\\n\\n`;\n  output += `Released:\
    \ ${notes.date}\\n\\n`;\n\n  if (notes.breaking.length > 0) {\n    output += '##\
    \ \u26A0\uFE0F Breaking Changes\\n\\n';\n    notes.breaking.forEach(item => output\
    \ += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  if (notes.features.length\
    \ > 0) {\n    output += '## \u2728 Features\\n\\n';\n    notes.features.forEach(item\
    \ => output += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  if (notes.fixes.length\
    \ > 0) {\n    output += '## \U0001F41B Bug Fixes\\n\\n';\n    notes.fixes.forEach(item\
    \ => output += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  if (notes.security.length\
    \ > 0) {\n    output += '## \U0001F512 Security\\n\\n';\n    notes.security.forEach(item\
    \ => output += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  if (notes.performance.length\
    \ > 0) {\n    output += '## \u26A1 Performance\\n\\n';\n    notes.performance.forEach(item\
    \ => output += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  if (notes.dependencies.length\
    \ > 0) {\n    output += '## \U0001F4E6 Dependencies\\n\\n';\n    notes.dependencies.forEach(item\
    \ => output += `- ${item}\\n`);\n    output += '\\n';\n  }\n\n  return output;\n\
    }\n```\n\n### 8.2 Rollback Procedures\n\n#### Automated Rollback **[REQUIRED]**\n\
    ```yaml"
  kubernetes/rollback/policy.yaml: "apiVersion: flagger.app/v1beta1\nkind: AlertProvider\n\
    metadata:\n  name: rollback-webhook\n  namespace: flagger-system\nspec:\n  type:\
    \ webhook\n  address: http://rollback-controller.platform/webhook\n  secret:\n\
    \    name: rollback-webhook-secret\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n\
    \  name: rollback-policy\n  namespace: platform\ndata:\n  policy.yaml: |\n   \
    \ rollback_triggers:\n      - name: high_error_rate\n        condition: \"error_rate\
    \ > 5%\"\n        duration: 5m\n        action: immediate\n\n      - name: latency_degradation\n\
    \        condition: \"p99_latency > 2 * baseline\"\n        duration: 10m\n  \
    \      action: immediate\n\n      - name: memory_leak\n        condition: \"memory_usage_rate\
    \ > 10MB/min\"\n        duration: 30m\n        action: scheduled\n\n      - name:\
    \ crash_loop\n        condition: \"restart_count > 5\"\n        duration: 15m\n\
    \        action: immediate\n\n    rollback_procedures:\n      immediate:\n   \
    \     - pause_deployments\n        - revert_to_previous\n        - notify_oncall\n\
    \        - create_incident\n\n      scheduled:\n        - notify_team\n      \
    \  - schedule_rollback\n        - monitor_metrics\n        - prepare_hotfix\n\
    ```\n\n#### Manual Rollback Procedures **[REQUIRED]**\n```bash\n#!/bin/bash"
  scripts/rollback.sh: set -euo pipefail
  Colors for output: 'RED=''\033[0;31m''

    GREEN=''\033[0;32m''

    YELLOW=''\033[1;33m''

    NC=''\033[0m'''
  Configuration: "NAMESPACE=${NAMESPACE:-production}\nSERVICE=${SERVICE:-}\nVERSION=${VERSION:-}\n\
    \nfunction usage() {\n    echo \"Usage: $0 -s SERVICE -v VERSION [-n NAMESPACE]\"\
    \n    echo \"  -s SERVICE    Service name to rollback\"\n    echo \"  -v VERSION\
    \    Version to rollback to\"\n    echo \"  -n NAMESPACE  Kubernetes namespace\
    \ (default: production)\"\n    exit 1\n}\n\nfunction log() {\n    echo -e \"${GREEN}[$(date\
    \ +'%Y-%m-%d %H:%M:%S')]${NC} $1\"\n}\n\nfunction error() {\n    echo -e \"${RED}[$(date\
    \ +'%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1\" >&2\n}\n\nfunction warning() {\n   \
    \ echo -e \"${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1\"\n}"
  Parse arguments: "while getopts \"s:v:n:h\" opt; do\n    case ${opt} in\n      \
    \  s) SERVICE=$OPTARG ;;\n        v) VERSION=$OPTARG ;;\n        n) NAMESPACE=$OPTARG\
    \ ;;\n        h) usage ;;\n        *) usage ;;\n    esac\ndone"
  Validate arguments: "if [[ -z \"$SERVICE\" ]] || [[ -z \"$VERSION\" ]]; then\n \
    \   error \"Service and version are required\"\n    usage\nfi"
  Main rollback procedure: "function main() {\n    log \"Starting rollback for $SERVICE\
    \ to version $VERSION in namespace $NAMESPACE\"\n\n    # 1. Check current deployment\n\
    \    log \"Checking current deployment...\"\n    CURRENT_VERSION=$(kubectl get\
    \ deployment \"$SERVICE\" -n \"$NAMESPACE\" \\\n        -o jsonpath='{.spec.template.spec.containers[0].image}'\
    \ | cut -d: -f2)\n    log \"Current version: $CURRENT_VERSION\"\n\n    if [[ \"\
    $CURRENT_VERSION\" == \"$VERSION\" ]]; then\n        warning \"Already at version\
    \ $VERSION, nothing to do\"\n        exit 0\n    fi\n\n    # 2. Create backup\
    \ of current state\n    log \"Creating backup of current deployment...\"\n   \
    \ kubectl get deployment \"$SERVICE\" -n \"$NAMESPACE\" -o yaml > \\\n       \
    \ \"/tmp/${SERVICE}-${CURRENT_VERSION}-backup.yaml\"\n\n    # 3. Check if target\
    \ version exists\n    log \"Verifying target version exists...\"\n    if ! docker\
    \ manifest inspect \"${REGISTRY}/${SERVICE}:${VERSION}\" &>/dev/null; then\n \
    \       error \"Version $VERSION not found in registry\"\n        exit 1\n   \
    \ fi\n\n    # 4. Update deployment\n    log \"Rolling back to version $VERSION...\"\
    \n    kubectl set image \"deployment/$SERVICE\" \\\n        \"${SERVICE}=${REGISTRY}/${SERVICE}:${VERSION}\"\
    \ \\\n        -n \"$NAMESPACE\" \\\n        --record\n\n    # 5. Monitor rollout\n\
    \    log \"Monitoring rollout status...\"\n    if ! kubectl rollout status \"\
    deployment/$SERVICE\" -n \"$NAMESPACE\" --timeout=10m; then\n        error \"\
    Rollback failed, attempting to restore previous version\"\n        kubectl apply\
    \ -f \"/tmp/${SERVICE}-${CURRENT_VERSION}-backup.yaml\"\n        exit 1\n    fi\n\
    \n    # 6. Verify health\n    log \"Verifying service health...\"\n    sleep 30\
    \  # Give pods time to stabilize\n\n    READY_REPLICAS=$(kubectl get deployment\
    \ \"$SERVICE\" -n \"$NAMESPACE\" \\\n        -o jsonpath='{.status.readyReplicas}')\n\
    \    DESIRED_REPLICAS=$(kubectl get deployment \"$SERVICE\" -n \"$NAMESPACE\"\
    \ \\\n        -o jsonpath='{.spec.replicas}')\n\n    if [[ \"$READY_REPLICAS\"\
    \ != \"$DESIRED_REPLICAS\" ]]; then\n        error \"Not all replicas are ready:\
    \ $READY_REPLICAS/$DESIRED_REPLICAS\"\n        exit 1\n    fi\n\n    # 7. Run\
    \ smoke tests\n    log \"Running smoke tests...\"\n    if command -v smoke-test\
    \ &>/dev/null; then\n        smoke-test --service \"$SERVICE\" --namespace \"\
    $NAMESPACE\"\n    else\n        warning \"Smoke test command not found, skipping\"\
    \n    fi\n\n    # 8. Update incident\n    log \"Updating incident tracking...\"\
    \n    if [[ -n \"${INCIDENT_ID:-}\" ]]; then\n        incident-cli update \"$INCIDENT_ID\"\
    \ \\\n            --status \"mitigated\" \\\n            --comment \"Rolled back\
    \ $SERVICE from $CURRENT_VERSION to $VERSION\"\n    fi\n\n    log \"Rollback completed\
    \ successfully!\"\n    log \"Service $SERVICE is now running version $VERSION\"\
    \n}"
  Execute main function: 'main

    ```


    ### 8.3 Deployment Monitoring


    #### Deployment Metrics **[REQUIRED]**

    ```yaml'
  monitoring/deployment-dashboard.json: "{\n  \"dashboard\": {\n    \"title\": \"\
    Deployment Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Deployment\
    \ Frequency\",\n        \"targets\": [\n          {\n            \"expr\": \"\
    sum(rate(deployments_total[1h])) by (service)\",\n            \"legendFormat\"\
    : \"{{ service }}\"\n          }\n        ]\n      },\n      {\n        \"title\"\
    : \"Deployment Success Rate\",\n        \"targets\": [\n          {\n        \
    \    \"expr\": \"sum(rate(deployments_total{status=\\\"success\\\"}[1h])) / sum(rate(deployments_total[1h]))\
    \ * 100\",\n            \"legendFormat\": \"Success Rate %\"\n          }\n  \
    \      ]\n      },\n      {\n        \"title\": \"Mean Time to Recovery\",\n \
    \       \"targets\": [\n          {\n            \"expr\": \"avg(deployment_rollback_duration_seconds)\
    \ by (service)\",\n            \"legendFormat\": \"{{ service }}\"\n         \
    \ }\n        ]\n      },\n      {\n        \"title\": \"Change Failure Rate\"\
    ,\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(deployments_total{status=\\\
    \"failed\\\"}[7d])) / sum(rate(deployments_total[7d])) * 100\",\n            \"\
    legendFormat\": \"Failure Rate %\"\n          }\n        ]\n      }\n    ]\n \
    \ }\n}\n```\n\n---"
  Implementation Guidelines: '### Adoption Strategy

    1. **Assessment Phase**: Evaluate current DevOps maturity

    2. **Foundation Phase**: Implement core IaC and CI/CD standards

    3. **Platform Phase**: Build internal developer platform

    4. **Optimization Phase**: Add SRE practices and chaos engineering

    5. **Excellence Phase**: Achieve full automation and self-service


    ### Tool Selection Criteria

    - **Compatibility**: Integration with existing tools

    - **Scalability**: Support for growth

    - **Community**: Active development and support

    - **Security**: Built-in security features

    - **Cost**: TCO including licenses and operations


    ### Success Metrics

    - **Deployment Frequency**: Daily deployments per service

    - **Lead Time**: < 1 hour from commit to production

    - **MTTR**: < 30 minutes for critical services

    - **Change Failure Rate**: < 5% of deployments

    - **Platform Adoption**: > 90% of teams using IDP


    ---


    **End of DevOps and Platform Engineering Standards**'
metadata:
  version: 1.0.0
  last_updated: '2025-06-20T05:11:54.002518'
  source: williamzujkowski/standards/docs/standards/DEVOPS_PLATFORM_STANDARDS.md
  checksum: 8cfd1d13de65859aea5b76daeb63f81bb5be3fb1af90079905287ac7398b7fde
